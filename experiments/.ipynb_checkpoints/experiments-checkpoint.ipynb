{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython==0.28 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (0.28)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install Cython==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (0.39.0)\n",
      "Requirement already satisfied: cloudpickle in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: numpy in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (1.20.3)\n",
      "Requirement already satisfied: scipy in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (1.6.3)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (4.61.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: scikit-learn in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (0.24.2)\n",
      "Requirement already satisfied: pandas in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (1.2.4)\n",
      "Requirement already satisfied: numba in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from shap) (0.54.0)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from numba->shap) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from numba->shap) (52.0.0.post20210125)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from scikit-learn->shap) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from scikit-learn->shap) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anchor in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: pyasn1-modules in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (0.2.8)\n",
      "Requirement already satisfied: netaddr!=0.7.16,>=0.7.12 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (0.8.0)\n",
      "Requirement already satisfied: pycadf!=2.0.0,>=1.1.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (3.1.1)\n",
      "Requirement already satisfied: oslo.config>=3.7.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (8.7.1)\n",
      "Requirement already satisfied: ldap3>=0.9.8.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (2.9.1)\n",
      "Requirement already satisfied: stevedore>=1.5.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (3.4.0)\n",
      "Requirement already satisfied: WebOb>=1.2.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (1.8.7)\n",
      "Requirement already satisfied: Paste in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (3.5.0)\n",
      "Requirement already satisfied: requests!=2.9.0,>=2.8.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (2.25.1)\n",
      "Requirement already satisfied: oslo.utils>=3.5.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (4.10.0)\n",
      "Requirement already satisfied: oslo.messaging>=4.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (12.9.0)\n",
      "Requirement already satisfied: cryptography>=1.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (3.4.7)\n",
      "Requirement already satisfied: pecan>=1.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (1.4.0)\n",
      "Requirement already satisfied: pyasn1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from anchor) (0.4.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from cryptography>=1.0->anchor) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.0->anchor) (2.20)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.config>=3.7.0->anchor) (4.3.0)\n",
      "Requirement already satisfied: oslo.i18n>=3.15.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.config>=3.7.0->anchor) (5.0.1)\n",
      "Requirement already satisfied: debtcollector>=1.2.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.config>=3.7.0->anchor) (2.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.config>=3.7.0->anchor) (5.4.1)\n",
      "Requirement already satisfied: rfc3986>=1.2.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.config>=3.7.0->anchor) (1.5.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from debtcollector>=1.2.0->oslo.config>=3.7.0->anchor) (5.6.0)\n",
      "Requirement already satisfied: wrapt>=1.7.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from debtcollector>=1.2.0->oslo.config>=3.7.0->anchor) (1.12.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from debtcollector>=1.2.0->oslo.config>=3.7.0->anchor) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->oslo.config>=3.7.0->anchor) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->oslo.config>=3.7.0->anchor) (3.4.1)\n",
      "Requirement already satisfied: futurist>=1.2.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (2.3.0)\n",
      "Requirement already satisfied: oslo.log>=3.36.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (4.6.0)\n",
      "Requirement already satisfied: oslo.serialization!=2.19.1,>=2.18.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (4.2.0)\n",
      "Requirement already satisfied: amqp>=2.5.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (5.0.6)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (4.2.2)\n",
      "Requirement already satisfied: oslo.service!=1.28.1,>=1.24.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (2.6.0)\n",
      "Requirement already satisfied: oslo.middleware>=3.31.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (4.4.0)\n",
      "Requirement already satisfied: kombu>=4.6.6 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (5.1.0)\n",
      "Requirement already satisfied: oslo.metrics>=0.2.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.messaging>=4.0.0->anchor) (0.3.0)\n",
      "Requirement already satisfied: vine==5.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from amqp>=2.5.2->oslo.messaging>=4.0.0->anchor) (5.0.0)\n",
      "Requirement already satisfied: cached-property in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from kombu>=4.6.6->oslo.messaging>=4.0.0->anchor) (1.5.2)\n",
      "Requirement already satisfied: oslo.context>=2.20.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.log>=3.36.0->oslo.messaging>=4.0.0->anchor) (3.3.1)\n",
      "Requirement already satisfied: pyinotify>=0.9.6 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.log>=3.36.0->oslo.messaging>=4.0.0->anchor) (0.9.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.log>=3.36.0->oslo.messaging>=4.0.0->anchor) (2.8.1)\n",
      "Requirement already satisfied: prometheus-client>=0.6.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.metrics>=0.2.1->oslo.messaging>=4.0.0->anchor) (0.10.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.middleware>=3.31.0->oslo.messaging>=4.0.0->anchor) (3.2.0)\n",
      "Requirement already satisfied: statsd>=3.2.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.middleware>=3.31.0->oslo.messaging>=4.0.0->anchor) (3.3.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.middleware>=3.31.0->oslo.messaging>=4.0.0->anchor) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from Jinja2>=2.10->oslo.middleware>=3.31.0->oslo.messaging>=4.0.0->anchor) (1.1.1)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.serialization!=2.19.1,>=2.18.0->oslo.messaging>=4.0.0->anchor) (1.0.2)\n",
      "Requirement already satisfied: pytz>=2013.6 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.serialization!=2.19.1,>=2.18.0->oslo.messaging>=4.0.0->anchor) (2021.1)\n",
      "Requirement already satisfied: greenlet>=0.4.15 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (1.1.1)\n",
      "Requirement already satisfied: fixtures>=3.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (3.0.0)\n",
      "Requirement already satisfied: oslo.concurrency>=3.25.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (4.4.0)\n",
      "Requirement already satisfied: PasteDeploy>=1.5.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (2.1.1)\n",
      "Requirement already satisfied: Yappi>=1.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (1.3.2)\n",
      "Requirement already satisfied: eventlet>=0.25.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (0.31.1)\n",
      "Requirement already satisfied: Routes>=2.3.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (2.5.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dnspython<2.0.0,>=1.15.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from eventlet>=0.25.2->oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (1.16.0)\n",
      "Requirement already satisfied: testtools>=0.9.22 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from fixtures>=3.0.0->oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (2.5.0)\n",
      "Requirement already satisfied: fasteners>=0.7.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.concurrency>=3.25.0->oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (0.16.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.utils>=3.5.0->anchor) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.4 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.utils>=3.5.0->anchor) (20.9)\n",
      "Requirement already satisfied: netifaces>=0.10.4 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.utils>=3.5.0->anchor) (0.11.0)\n",
      "Requirement already satisfied: iso8601>=0.1.11 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from oslo.utils>=3.5.0->anchor) (0.1.16)\n",
      "Requirement already satisfied: setuptools in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from Paste->anchor) (52.0.0.post20210125)\n",
      "Requirement already satisfied: WebTest>=1.3.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from pecan>=1.0.0->anchor) (3.0.0)\n",
      "Requirement already satisfied: logutils>=0.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from pecan>=1.0.0->anchor) (0.3.5)\n",
      "Requirement already satisfied: Mako>=0.4.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from pecan>=1.0.0->anchor) (1.1.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from requests!=2.9.0,>=2.8.1->anchor) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from requests!=2.9.0,>=2.8.1->anchor) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from requests!=2.9.0,>=2.8.1->anchor) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from requests!=2.9.0,>=2.8.1->anchor) (4.0.0)\n",
      "Requirement already satisfied: repoze.lru>=0.3 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from Routes>=2.3.1->oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (0.7)\n",
      "Requirement already satisfied: extras>=1.0.0 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from testtools>=0.9.22->fixtures>=3.0.0->oslo.service!=1.28.1,>=1.24.0->oslo.messaging>=4.0.0->anchor) (1.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from WebTest>=1.3.1->pecan>=1.0.0->anchor) (4.9.3)\n",
      "Requirement already satisfied: waitress>=0.8.5 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from WebTest>=1.3.1->pecan>=1.0.0->anchor) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from beautifulsoup4->WebTest>=1.3.1->pecan>=1.0.0->anchor) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rbo in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in /cshome/alamanik/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages (from rbo) (1.20.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement dlukes_rbo (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for dlukes_rbo\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dlukes_rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.198955Z",
     "start_time": "2020-10-28T08:15:32.480547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "console.log('Starting front end url_querystring_target comm target');\n",
       "const comm = Jupyter.notebook.kernel.comm_manager.new_comm('url_querystring_target', {'init': 1});\n",
       "comm.send({'ipyparams_browser_url': window.location.href});\n",
       "console.log('Sent window.location.href on url_querystring_target comm target');\n",
       "\n",
       "comm.on_msg(function(msg) {\n",
       "    console.log(msg.content.data);\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/cshome/alamanik/.ipython', '/cshome/alamanik/barbetest/', '/cshome/alamanik/barbetest/sigdirect/']\n",
      "Hi 1 <module 'experiments_config' from '/cshome/alamanik/barbetest/experiments/experiments_config.py'>\n",
      "Hi 2222222222222222222222222222\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 1\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "#import pathos\n",
    "# from multiprocessing import Pool, TimeoutError\n",
    "from importlib import reload  \n",
    "import ipykernel\n",
    "import requests\n",
    "# Alternative that works for both Python 2 and 3:\n",
    "from requests.compat import urljoin\n",
    "from notebook.notebookapp import list_running_servers\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "# np.random.seed(seed=RANDOM_SEED)\n",
    "const_random_state = RandomState(RANDOM_SEED)\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger().setLevel('CRITICAL')\n",
    "\n",
    "import experiments_config\n",
    "import ipyparams\n",
    "\n",
    "#sys.path.append(experiments_config.SIGDIRECT_PATH)\n",
    "# from associative_classifier import AssociativeClassifier\n",
    "\n",
    "############ importing LIME   ############\n",
    "# if experiments_config.LIME_PATH not in sys.path:\n",
    "#     sys.path.append(experiments_config.LIME_PATH)\n",
    "\n",
    "LIME_PATH = '/cshome/alamanik/barbetest/'\n",
    "SIGDIRECT_PATH = '/cshome/alamanik/barbetest/sigdirect/'\n",
    "sys.path.append(LIME_PATH)\n",
    "sys.path.append(SIGDIRECT_PATH)\n",
    "\n",
    "print(sys.path[-3:])\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "############ importing XLIME  ############\n",
    "# if experiments_config.XLIME_PATH not in sys.path:\n",
    "#     sys.path.append(experiments_config.XLIME_PATH)\n",
    "# import xlime as xlime\n",
    "# import xlime.lime_tabular\n",
    "############ importing SHAP   ############\n",
    "#import shap\n",
    "############ importing Anchor ############\n",
    "import anchor\n",
    "#import anchor.anchor_tabular\n",
    "\n",
    "############ importing RBO metric ############\n",
    "import rbo\n",
    "#import dlukes_rbo\n",
    "\n",
    "print('Hi 1', experiments_config)\n",
    "print('Hi 2222222222222222222222222222')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'base_url': '/', 'hostname': 'localhost', 'notebook_dir': '/cshome/alamanik/barbetest', 'password': True, 'pid': 1005449, 'port': 8078, 'secure': False, 'sock': '', 'token': '12345678', 'url': 'http://localhost:8078/'}]\n",
      "backup_file\n"
     ]
    }
   ],
   "source": [
    "def _reload_libs():\n",
    "    global lime\n",
    "#     xlime = reload(xlime)\n",
    "#     xlime.lime_tabular = reload(xlime.lime_tabular)\n",
    "    lime = reload(lime)\n",
    "    lime.lime_tabular = reload(lime.lime_tabular)\n",
    "\n",
    "def get_notebook_name():\n",
    "    kernel_id = re.search('kernel-(.*).json',\n",
    "                          ipykernel.connect.get_connection_file()).group(1)\n",
    "    servers = list_running_servers()\n",
    "    print(list(servers))\n",
    "    for ss in servers:\n",
    "        response = requests.get(urljoin(ss['url'], 'api/sessions'),\n",
    "                                params={'token': ss.get('token', '')})\n",
    "        for nn in json.loads(response.text):\n",
    "            if nn['kernel']['id'] == kernel_id:\n",
    "                return nn['notebook']['path'].split('/')[-1]\n",
    "    return \"backup_file\"\n",
    "print(get_notebook_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.231244Z",
     "start_time": "2020-10-28T08:15:38.201534Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_df(filename, has_index, header_index=None):\n",
    "    df = pd.read_csv(filename, sep=',', header=header_index, na_values='?')\n",
    "    if has_index:\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "    return df\n",
    "\n",
    "def get_test_df(filename, has_index, header_index=None):\n",
    "    df = pd.read_csv(filename, sep=',', header=header_index, na_values='?')\n",
    "    if has_index:\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(train_df, test_df, class_index, dataset_name):\n",
    "    # removing class label, so we can call get_dummies on the rest\n",
    "    train_df.rename(columns={list(train_df.columns)[class_index]:'class'}, inplace=True)\n",
    "    print(train_df)\n",
    "    print({list(train_df.columns)[class_index]:'class'})\n",
    "    train_class = train_df['class']\n",
    "    print(train_class)\n",
    "    train_df.drop(columns=['class'], inplace=True)\n",
    "    print(train_df)\n",
    "    \n",
    "    test_df.rename(columns={list(test_df.columns)[class_index]:'class'}, inplace=True)\n",
    "    test_class = test_df['class']\n",
    "    test_df.drop(columns=['class'], inplace=True)\n",
    "\n",
    "    # process categorical data\n",
    "    infer_types = []\n",
    "    df = pd.concat([train_df, test_df])     #Now they no class column\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype=='object':\n",
    "            infer_types.append(\"{}_CAT\".format(column))\n",
    "        else:\n",
    "            infer_types.append(\"{}_NUM\".format(column))\n",
    "    datasets_info_dict[dataset_name]['_NUM'] = sum(['_NUM' in x for x in infer_types])\n",
    "    datasets_info_dict[dataset_name]['_CAT'] = sum(['_CAT' in x for x in infer_types])\n",
    "#     print('INFER_TYPES:', infer_types)\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    train_df = df[:train_df.shape[0]]\n",
    "    test_df  = df[train_df.shape[0]:]\n",
    "\n",
    "    print(train_df)\n",
    "    print(test_df)\n",
    "    assert set(train_df.columns)==set(test_df.columns)\n",
    "\n",
    "    # process numerical data (standardization is independent[?] for train/test splits)\n",
    "    continuous_column_names = [x for x in list(train_df.columns) if not '_' in str(x)]\n",
    "    print(continuous_column_names)\n",
    "    for column in continuous_column_names:\n",
    "        # standardazing the column\n",
    "        scaler = StandardScaler()\n",
    "        train_df[column] = scaler.fit_transform(train_df[column].to_numpy().reshape((-1,1)))\n",
    "        test_df[column]  = scaler.transform(test_df[column].to_numpy().reshape((-1,1)))\n",
    "        \n",
    "        # set NaN to 0\n",
    "        train_df[column].fillna(0., inplace=True)\n",
    "        test_df[column].fillna(0., inplace=True)\n",
    "        \n",
    "    train_df['class'] = train_class\n",
    "    test_df['class']  = test_class\n",
    "    \n",
    "    print(train_df)\n",
    "    print(test_df)\n",
    "    return train_df, test_df\n",
    "    \n",
    "def get_data(dataset_name, dataset_info):\n",
    "    # input: name of the dataset, and a dictionary containing its info\n",
    "    # process input arguments\n",
    "    ## TA: If file_counts is 2, then train file and test filenames are separated. If 1, then 1 single fine\n",
    "    file_counts  = 2 if dataset_info['SEPARATE_FILES'] else 1\n",
    "    class_index  = dataset_info['CLASS_INDEX']\n",
    "    has_index    = dataset_info['HAS_INDEX']\n",
    "    header_index = dataset_info['HEADER_ROW_NUMBER']\n",
    "    \n",
    "    # set train/test filenames\n",
    "    if file_counts==2:\n",
    "        filenames = [os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['TRAIN_FILENAME']), \n",
    "                     os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['TEST_FILENAME'])]\n",
    "    else:\n",
    "        filenames = [os.path.join(DATA_ROOT_DIR, dataset_info['FOLDER_NAME'], dataset_info['COMBINED_FILENAME'])]\n",
    "\n",
    "\n",
    "    print(filenames)\n",
    "    filenames = ['/cshome/alamanik/barbetest/dataset/glass.data']\n",
    "    \n",
    "    if file_counts==1:\n",
    "        # load from file\n",
    "        df = get_train_df(filenames[0], has_index, header_index)\n",
    "        # shuffle\n",
    "        df = df.sample(frac=1, random_state=const_random_state)\n",
    "\n",
    "        train_size = int(df.shape[0] * TRAIN_RATIO)\n",
    "        train_df = df.iloc[:train_size]\n",
    "        test_df  = df.iloc[train_size:]\n",
    "        \n",
    "    else:\n",
    "        # load from file\n",
    "        train_df = get_train_df(filenames[0], has_index, header_index)\n",
    "        test_df  = get_test_df(filenames[1], has_index, header_index)\n",
    "        # shuffle\n",
    "        train_df = train_df.sample(frac=1, random_state=const_random_state)\n",
    "        test_df  = test_df.sample(frac=1, random_state=const_random_state)\n",
    "\n",
    "    # some of the preprocessing (one hot encoding + missing values + standardisation)\n",
    "    dataset_info['initial_features'] = train_df.shape[1] -1\n",
    "    \n",
    "    train_df, test_df = preprocess_data(train_df, test_df, class_index, dataset_name)\n",
    "    \n",
    "    dataset_info['features'] = train_df.shape[1] -1\n",
    "    dataset_info['initial_train_size'] = train_df.shape[0]\n",
    "    dataset_info['initial_test_size'] = test_df.shape[0]\n",
    "    dataset_info['original_train_df'] = train_df.copy()\n",
    "    dataset_info['original_test_df']  = test_df.copy()\n",
    "    \n",
    "    if shrink_train_size and train_df.shape[0]>desired_train_size:\n",
    "        train_df = train_df[:desired_train_size]\n",
    "    if shrink_test_size  and test_df.shape[0]>desired_test_size:\n",
    "        test_df = test_df[:desired_test_size]\n",
    "    dataset_info['train_size'] = train_df.shape[0]\n",
    "    dataset_info['test_size'] = test_df.shape[0]\n",
    "        \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.335331Z",
     "start_time": "2020-10-28T08:15:38.233736Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf(train_df, clf_type, info=None):\n",
    "    if clf_type.lower()=='dt':\n",
    "        return get_clf_dt(train_df, info['max_explanation_size'])\n",
    "    if clf_type.lower()=='lr':\n",
    "        return get_clf_lr(train_df, info['max_explanation_size'])\n",
    "    raise Exception('Wrong interpretable model, select from \"lr\", \"dt\"')\n",
    "\n",
    "def get_features(classifier_type, clf, row, info_dict):\n",
    "    if classifier_type=='sd':\n",
    "        clf_features = get_features_sd(clf, row)\n",
    "    elif classifier_type=='dt':\n",
    "        clf_features = get_features_dt(clf, row)\n",
    "    elif classifier_type=='lr':\n",
    "        clf_features = get_features_lr(clf, row, \n",
    "                                       info_dict['max_features'], \n",
    "                                       info_dict['num_labels'], \n",
    "                                       info_dict['predicted_label_index'])\n",
    "    else:\n",
    "        print('Incorrect classifier type. terminating ...', classifier_type)\n",
    "        raise Exception(\"Incorrect classifier type\")\n",
    "    return clf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.449839Z",
     "start_time": "2020-10-28T08:15:38.337561Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_dt(train_df, max_depth):\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(random_state=const_random_state, max_depth=max_depth)\n",
    "    clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "    return clf\n",
    "\n",
    "def test_classifier_dt(clf, test_df):\n",
    "    predictions = clf.predict(test_df.drop('class', axis=1))    \n",
    "    acc = sklearn.metrics.accuracy_score(test_df['class'], predictions)\n",
    "    return acc\n",
    "\n",
    "def get_features_dt(clf, row):\n",
    "    feature = clf.tree_.feature\n",
    "    print('Inside: get_features_dt, feature = ', feature)\n",
    "    leave_id = clf.apply(row.values.reshape(1, -1))\n",
    "    #print(leave_id)\n",
    "    node_indicator = clf.decision_path([row])\n",
    "    #print(node_indicator)\n",
    "    features = OrderedDict() # using as an ordered set\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "#     node_index = node_indicator.indices[:] #node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "    #print(node_index)\n",
    "\n",
    "    for node_id in node_index:\n",
    "        if leave_id[0] == node_id:  # <-- changed != to ==\n",
    "            continue # <-- comment out\n",
    "        else: # < -- added else to iterate through decision nodes\n",
    "#             features.append(feature[node_id]+1)\n",
    "            if feature[node_id]+1 not in features:\n",
    "                features[feature[node_id]+1] = None\n",
    "#             else:\n",
    "#                 print('redundant!!!')\n",
    "    #print(features)\n",
    "    final_features = [*features]\n",
    "    print('final_features = ', final_features)\n",
    "#     print(final_features)\n",
    "    return final_features\n",
    "\n",
    "def get_dt_avg_explanation(dt_clf, train_df):\n",
    "    # find the depth for each training instance, and then return the average among them\n",
    "#     decision_paths = dt_clf.decision_path(train_df).toarray()\n",
    "#     uniques = np.unique(decision_paths, axis=1)\n",
    "#     print(decision_paths[:5])\n",
    "#     print(np.count_nonzero(decision_paths, axis=1).max()-1, np.count_nonzero(decision_paths, axis=1).min()-1)\n",
    "#     return np.count_nonzero(decision_paths, axis=1).mean() - 1 # they all have an extra node which is the leaf\n",
    "    lens = 0\n",
    "    print('*** ', dt_clf)\n",
    "    print('*** ', train_df)\n",
    "    for _,row in train_df.iterrows():\n",
    "        exp = get_features_dt(dt_clf, row)\n",
    "        lens += len(exp)\n",
    "    return lens/train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.641078Z",
     "start_time": "2020-10-28T08:15:38.451740Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_lr(train_df, max_features):\n",
    "    try_cs1 = np.arange(1.,0,-.1)\n",
    "    try_cs2 = np.arange(.1,0,-.01)\n",
    "    try_cs3 = np.arange(.01,0,-.001)\n",
    "    \n",
    "    done = False\n",
    "    for c in try_cs1:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "    if done:\n",
    "        return temp_clf\n",
    "    for c in try_cs2:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "    if done:\n",
    "        return temp_clf\n",
    "    for c in try_cs3:\n",
    "        temp_clf = sklearn.linear_model.LogisticRegression(random_state=const_random_state, penalty='l1', fit_intercept=True, C=c, n_jobs=-1, solver='liblinear')\n",
    "        temp_clf.fit(train_df.drop('class', axis=1), train_df['class'])\n",
    "        lengths = [len(x.nonzero()[0]) for x in temp_clf.coef_]\n",
    "        if np.max(lengths) <= max_features:\n",
    "            done = True\n",
    "            break\n",
    "#     print('c:', c)\n",
    "    return temp_clf\n",
    "    \n",
    "def test_classifier_lr(clf, test_df):\n",
    "    predictions = clf.predict(test_df.drop('class', axis=1))    \n",
    "    acc = sklearn.metrics.accuracy_score(test_df['class'], predictions)\n",
    "    return acc\n",
    "\n",
    "def get_features_lr(clf, row, max_features, num_classes, label_index):\n",
    "    if num_classes<=2:\n",
    "        idx = 0\n",
    "    else:\n",
    "        idx = label_index\n",
    "    all_params = clf.coef_\n",
    "    return set(np.where(all_params[idx]!=0.)[0]+1)\n",
    "#     return set(np.argsort(all_params[idx])[:].tolist())\n",
    "\n",
    "def get_lr_avg_explanation(clf, train_df):\n",
    "    return max([len(x.nonzero()[0]) for x in clf.coef_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:38.861488Z",
     "start_time": "2020-10-28T08:15:38.642975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glass': {'FOLDER_NAME': 'glass',\n",
       "  'SEPARATE_FILES': False,\n",
       "  'TRAIN_FILENAME': None,\n",
       "  'TEST_FILENAME': None,\n",
       "  'COMBINED_FILENAME': 'glass.data',\n",
       "  'CLASS_INDEX': -1,\n",
       "  'HAS_INDEX': True,\n",
       "  'HEADER_ROW_NUMBER': None,\n",
       "  'initial_features': 9,\n",
       "  '_NUM': 9,\n",
       "  '_CAT': 0,\n",
       "  'features': 9,\n",
       "  'initial_train_size': 160,\n",
       "  'initial_test_size': 54,\n",
       "  'original_train_df':             1         2         3         4         5         6         7  \\\n",
       "  108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
       "  206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
       "  106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
       "  107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
       "  51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
       "  ..        ...       ...       ...       ...       ...       ...       ...   \n",
       "  201 -0.562204 -1.655068 -1.763019 -0.515066  3.101621  2.956999 -0.030414   \n",
       "  64   1.020031  0.093349  0.768756 -1.081261 -0.820840 -0.461183  0.414348   \n",
       "  15  -0.232953 -0.672298  0.633367 -0.436971  0.701124  0.081385 -0.383607   \n",
       "  41  -0.251245 -0.786573  0.552133 -0.495542  0.651630  0.094950 -0.220092   \n",
       "  176  0.206049  0.687582 -0.145120  0.207320 -0.375387 -0.705339  0.388186   \n",
       "  \n",
       "              8         9  class  \n",
       "  108 -0.369498  0.223975      2  \n",
       "  206  2.213128 -0.581329      7  \n",
       "  106  5.525627  2.237237      2  \n",
       "  107 -0.369498  1.834585      2  \n",
       "  51  -0.369498  0.525965      1  \n",
       "  ..        ...       ...    ...  \n",
       "  201 -0.369498 -0.581329      7  \n",
       "  64  -0.369498  0.123312      1  \n",
       "  15  -0.369498 -0.581329      1  \n",
       "  41  -0.369498 -0.581329      1  \n",
       "  176 -0.369498 -0.581329      6  \n",
       "  \n",
       "  [160 rows x 10 columns],\n",
       "  'original_test_df':             1         2         3         4         5         6         7  \\\n",
       "  109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
       "  80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
       "  52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
       "  26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
       "  76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
       "  43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
       "  24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
       "  3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
       "  182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
       "  49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
       "  149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
       "  131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
       "  209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
       "  30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
       "  121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
       "  175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
       "  115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
       "  188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
       "  8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
       "  60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
       "  128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
       "  1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
       "  57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
       "  22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
       "  61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
       "  63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
       "  7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
       "  210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
       "  141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
       "  86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
       "  96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
       "  68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
       "  50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
       "  142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
       "  157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
       "  156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
       "  139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
       "  146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
       "  101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
       "  20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
       "  178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
       "  25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
       "  134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
       "  71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
       "  129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
       "  144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
       "  192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
       "  79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
       "  133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
       "  203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
       "  137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
       "  72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
       "  140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
       "  37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
       "  \n",
       "              8         9  class  \n",
       "  109 -0.369498 -0.581329      2  \n",
       "  80  -0.369498 -0.581329      2  \n",
       "  52  -0.369498 -0.581329      1  \n",
       "  26  -0.369498 -0.581329      1  \n",
       "  76  -0.369498 -0.581329      2  \n",
       "  43  -0.369498 -0.581329      1  \n",
       "  24  -0.369498 -0.581329      1  \n",
       "  3   -0.369498 -0.581329      1  \n",
       "  182 -0.369498 -0.581329      6  \n",
       "  49  -0.369498 -0.581329      1  \n",
       "  149 -0.369498 -0.581329      3  \n",
       "  131 -0.369498  0.425302      2  \n",
       "  209  1.614258 -0.581329      7  \n",
       "  30  -0.369498  0.827954      1  \n",
       "  121 -0.369498  1.532596      2  \n",
       "  175 -0.369498  2.237237      5  \n",
       "  115 -0.369498 -0.581329      2  \n",
       "  188 -0.369498 -0.581329      7  \n",
       "  8   -0.369498 -0.581329      1  \n",
       "  60  -0.369498 -0.581329      1  \n",
       "  128  0.135798  1.129943      2  \n",
       "  1   -0.369498 -0.581329      1  \n",
       "  57  -0.369498 -0.581329      1  \n",
       "  22  -0.369498 -0.581329      1  \n",
       "  61   0.921815 -0.581329      1  \n",
       "  63  -0.369498 -0.581329      1  \n",
       "  7   -0.369498 -0.581329      1  \n",
       "  210  2.606136 -0.581329      7  \n",
       "  141 -0.201066  1.129943      2  \n",
       "  86  -0.369498 -0.581329      2  \n",
       "  96  -0.369498  0.928617      2  \n",
       "  68  -0.369498  1.029280      1  \n",
       "  50  -0.369498  1.029280      1  \n",
       "  142 -0.257210  1.935248      2  \n",
       "  157 -0.369498 -0.581329      3  \n",
       "  156 -0.369498 -0.581329      3  \n",
       "  139 -0.369498 -0.581329      2  \n",
       "  146 -0.369498 -0.581329      3  \n",
       "  101 -0.369498 -0.581329      2  \n",
       "  20  -0.369498  1.331270      1  \n",
       "  178 -0.369498 -0.581329      6  \n",
       "  25  -0.369498 -0.581329      1  \n",
       "  134 -0.369498 -0.581329      2  \n",
       "  71  -0.369498  2.639890      2  \n",
       "  129 -0.369498  1.230606      2  \n",
       "  144 -0.369498  1.834585      2  \n",
       "  192  0.379089  0.324638      7  \n",
       "  79  -0.369498 -0.581329      2  \n",
       "  133 -0.369498  0.928617      2  \n",
       "  203  2.830713 -0.581329      7  \n",
       "  137 -0.369498 -0.581329      2  \n",
       "  72  -0.369498 -0.581329      2  \n",
       "  140 -0.369498 -0.581329      2  \n",
       "  37  -0.369498 -0.581329      1  ,\n",
       "  'train_size': 100,\n",
       "  'test_size': 54,\n",
       "  'train_df':             1         2         3         4         5         6         7  \\\n",
       "  108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
       "  206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
       "  106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
       "  107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
       "  51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
       "  ..        ...       ...       ...       ...       ...       ...       ...   \n",
       "  132 -0.074425  0.036211  0.931222 -0.534590 -0.226903  0.081385 -0.540582   \n",
       "  193 -0.360995  1.544649 -1.763019  1.066374  0.428903 -0.705339 -0.292039   \n",
       "  153 -0.693295  0.024784  0.538594 -0.456495  0.020571  0.094950 -0.429391   \n",
       "  166  0.956010 -2.706404 -0.605443  0.207320  0.948598  0.081385  1.729012   \n",
       "  160 -0.016501 -0.078065  0.497978  0.168272 -0.659982  0.054257  0.008830   \n",
       "  \n",
       "              8         9  class  \n",
       "  108 -0.369498  0.223975      2  \n",
       "  206  2.213128 -0.581329      7  \n",
       "  106  5.525627  2.237237      2  \n",
       "  107 -0.369498  1.834585      2  \n",
       "  51  -0.369498  0.525965      1  \n",
       "  ..        ...       ...    ...  \n",
       "  132 -0.369498 -0.581329      2  \n",
       "  193  2.606136  0.223975      7  \n",
       "  153 -0.369498 -0.581329      3  \n",
       "  166 -0.369498 -0.581329      5  \n",
       "  160 -0.369498 -0.581329      3  \n",
       "  \n",
       "  [100 rows x 10 columns],\n",
       "  'test_df':             1         2         3         4         5         6         7  \\\n",
       "  109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
       "  80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
       "  52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
       "  26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
       "  76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
       "  43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
       "  24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
       "  3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
       "  182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
       "  49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
       "  149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
       "  131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
       "  209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
       "  30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
       "  121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
       "  175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
       "  115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
       "  188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
       "  8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
       "  60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
       "  128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
       "  1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
       "  57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
       "  22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
       "  61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
       "  63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
       "  7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
       "  210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
       "  141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
       "  86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
       "  96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
       "  68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
       "  50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
       "  142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
       "  157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
       "  156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
       "  139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
       "  146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
       "  101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
       "  20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
       "  178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
       "  25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
       "  134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
       "  71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
       "  129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
       "  144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
       "  192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
       "  79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
       "  133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
       "  203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
       "  137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
       "  72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
       "  140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
       "  37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
       "  \n",
       "              8         9  class  \n",
       "  109 -0.369498 -0.581329      2  \n",
       "  80  -0.369498 -0.581329      2  \n",
       "  52  -0.369498 -0.581329      1  \n",
       "  26  -0.369498 -0.581329      1  \n",
       "  76  -0.369498 -0.581329      2  \n",
       "  43  -0.369498 -0.581329      1  \n",
       "  24  -0.369498 -0.581329      1  \n",
       "  3   -0.369498 -0.581329      1  \n",
       "  182 -0.369498 -0.581329      6  \n",
       "  49  -0.369498 -0.581329      1  \n",
       "  149 -0.369498 -0.581329      3  \n",
       "  131 -0.369498  0.425302      2  \n",
       "  209  1.614258 -0.581329      7  \n",
       "  30  -0.369498  0.827954      1  \n",
       "  121 -0.369498  1.532596      2  \n",
       "  175 -0.369498  2.237237      5  \n",
       "  115 -0.369498 -0.581329      2  \n",
       "  188 -0.369498 -0.581329      7  \n",
       "  8   -0.369498 -0.581329      1  \n",
       "  60  -0.369498 -0.581329      1  \n",
       "  128  0.135798  1.129943      2  \n",
       "  1   -0.369498 -0.581329      1  \n",
       "  57  -0.369498 -0.581329      1  \n",
       "  22  -0.369498 -0.581329      1  \n",
       "  61   0.921815 -0.581329      1  \n",
       "  63  -0.369498 -0.581329      1  \n",
       "  7   -0.369498 -0.581329      1  \n",
       "  210  2.606136 -0.581329      7  \n",
       "  141 -0.201066  1.129943      2  \n",
       "  86  -0.369498 -0.581329      2  \n",
       "  96  -0.369498  0.928617      2  \n",
       "  68  -0.369498  1.029280      1  \n",
       "  50  -0.369498  1.029280      1  \n",
       "  142 -0.257210  1.935248      2  \n",
       "  157 -0.369498 -0.581329      3  \n",
       "  156 -0.369498 -0.581329      3  \n",
       "  139 -0.369498 -0.581329      2  \n",
       "  146 -0.369498 -0.581329      3  \n",
       "  101 -0.369498 -0.581329      2  \n",
       "  20  -0.369498  1.331270      1  \n",
       "  178 -0.369498 -0.581329      6  \n",
       "  25  -0.369498 -0.581329      1  \n",
       "  134 -0.369498 -0.581329      2  \n",
       "  71  -0.369498  2.639890      2  \n",
       "  129 -0.369498  1.230606      2  \n",
       "  144 -0.369498  1.834585      2  \n",
       "  192  0.379089  0.324638      7  \n",
       "  79  -0.369498 -0.581329      2  \n",
       "  133 -0.369498  0.928617      2  \n",
       "  203  2.830713 -0.581329      7  \n",
       "  137 -0.369498 -0.581329      2  \n",
       "  72  -0.369498 -0.581329      2  \n",
       "  140 -0.369498 -0.581329      2  \n",
       "  37  -0.369498 -0.581329      1  ,\n",
       "  'nclasses': 6,\n",
       "  'dt_len': 4.4,\n",
       "  'dt_acc': 0.6851851851851852,\n",
       "  'dt_clf': DecisionTreeClassifier(max_depth=5,\n",
       "                         random_state=RandomState(MT19937) at 0x7F2CC67B7160),\n",
       "  'lr_len': 5,\n",
       "  'lr_acc': 0.6481481481481481,\n",
       "  'lr_clf': LogisticRegression(C=0.20000000000000018, n_jobs=-1, penalty='l1',\n",
       "                     random_state=RandomState(MT19937) at 0x7F2CC67B7160,\n",
       "                     solver='liblinear')}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global variables\n",
    "shrink_train_size = True\n",
    "desired_train_size = 100\n",
    "\n",
    "shrink_test_size = True\n",
    "desired_test_size = 100\n",
    "\n",
    "# used if data is not already split\n",
    "TRAIN_RATIO = 0.75\n",
    "\n",
    "PROCESS_COUNT = 50\n",
    "\n",
    "# compute averages over all instances, or over the ones that the labels agree\n",
    "fidelity_division = True\n",
    "\n",
    "# how many runs to make sure fidelity is respected in BARBE/XLIME\n",
    "REPEAT_COUNT = 1\n",
    "\n",
    "DATA_ROOT_DIR = experiments_config.DATA_ROOT_DIR\n",
    "\n",
    "datasets_info_dict = experiments_config.all_datasets_info_dict.copy()\n",
    "\n",
    "datasets_info_dict = {\n",
    "                    'glass':experiments_config.all_datasets_info_dict['glass'],\n",
    "#                      'wine':experiments_config.all_datasets_info_dict['wine'],\n",
    "#                      'hungarian':experiments_config.all_datasets_info_dict['hungarian'],\n",
    "#                      'hepatitis':experiments_config.all_datasets_info_dict['hepatitis'],\n",
    "#                      'poker':experiments_config.all_datasets_info_dict['poker'],\n",
    "                     }\n",
    "\n",
    "\n",
    "info = {'max_explanation_size':5}\n",
    "\n",
    "remove_datasets = [\n",
    "    'online_shoppers_intention', # slow with 1k!\n",
    "    'breast-cancer', # very slow even in 200!, but good results!\n",
    "    'car', # has a small tree! f1=55, fidel:82\n",
    "    'nursery', # precision=1, recall=.5\n",
    "    'adult', # slow, 1k won't finish in 2h\n",
    "]\n",
    "# remove_datasets = ['online_shoppers_intention']\n",
    "\n",
    "for x in remove_datasets:\n",
    "    if x in  datasets_info_dict:\n",
    "        del datasets_info_dict[x]\n",
    "\n",
    "datasets_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of these experiments:\n",
    "I want to know more about the datasets I use, and have the enough knowledge about them so I could use different type of datasets (in terms of size/feature/complexity) in my experiments.\n",
    "\n",
    "Given say onnly 10 features, we can have feature spaces that are very complex, or very simple. The same thing can happen when we have 100 features. So, I need datasets that cover all such cases.\n",
    "\n",
    "\n",
    "I plan to use train sets to a) train the classifiers, and b) tune BARBE. Since this is my own training, I can limit the size of the train set used in the experiments to a reasonable number like 200.\n",
    "#### Obtaining General info about datasets\n",
    "1. Number of features, train instances, test instances\n",
    "2. \n",
    "\n",
    "#### Understanding how different interpretable classifiers work w.r.t. these datasets\n",
    "1. Classifier accuracy on these datasets\n",
    "2. info unique to each classifier:\n",
    "    2.1. DT: average depth of the tree \n",
    "    2.2. Rule-based: average number of rules for each instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.055328Z",
     "start_time": "2020-10-28T08:15:38.862801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('glass', {'FOLDER_NAME': 'glass', 'SEPARATE_FILES': False, 'TRAIN_FILENAME': None, 'TEST_FILENAME': None, 'COMBINED_FILENAME': 'glass.data', 'CLASS_INDEX': -1, 'HAS_INDEX': True, 'HEADER_ROW_NUMBER': None, 'initial_features': 9, '_NUM': 9, '_CAT': 0, 'features': 9, 'initial_train_size': 160, 'initial_test_size': 54, 'original_train_df':             1         2         3         4         5         6         7  \\\n",
      "108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
      "206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
      "106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
      "107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
      "51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "201 -0.562204 -1.655068 -1.763019 -0.515066  3.101621  2.956999 -0.030414   \n",
      "64   1.020031  0.093349  0.768756 -1.081261 -0.820840 -0.461183  0.414348   \n",
      "15  -0.232953 -0.672298  0.633367 -0.436971  0.701124  0.081385 -0.383607   \n",
      "41  -0.251245 -0.786573  0.552133 -0.495542  0.651630  0.094950 -0.220092   \n",
      "176  0.206049  0.687582 -0.145120  0.207320 -0.375387 -0.705339  0.388186   \n",
      "\n",
      "            8         9  class  \n",
      "108 -0.369498  0.223975      2  \n",
      "206  2.213128 -0.581329      7  \n",
      "106  5.525627  2.237237      2  \n",
      "107 -0.369498  1.834585      2  \n",
      "51  -0.369498  0.525965      1  \n",
      "..        ...       ...    ...  \n",
      "201 -0.369498 -0.581329      7  \n",
      "64  -0.369498  0.123312      1  \n",
      "15  -0.369498 -0.581329      1  \n",
      "41  -0.369498 -0.581329      1  \n",
      "176 -0.369498 -0.581329      6  \n",
      "\n",
      "[160 rows x 10 columns], 'original_test_df':             1         2         3         4         5         6         7  \\\n",
      "109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
      "80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
      "52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
      "26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
      "76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
      "43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
      "24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
      "3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
      "182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
      "49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
      "149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
      "131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
      "209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
      "30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
      "121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
      "175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
      "115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
      "188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
      "8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
      "60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
      "128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
      "1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
      "57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
      "22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
      "61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
      "63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
      "7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
      "210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
      "141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
      "86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
      "96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
      "68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
      "50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
      "142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
      "157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
      "156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
      "139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
      "146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
      "101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
      "20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
      "178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
      "25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
      "134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
      "71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
      "129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
      "144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
      "192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
      "79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
      "133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
      "203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
      "137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
      "72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
      "140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
      "37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
      "\n",
      "            8         9  class  \n",
      "109 -0.369498 -0.581329      2  \n",
      "80  -0.369498 -0.581329      2  \n",
      "52  -0.369498 -0.581329      1  \n",
      "26  -0.369498 -0.581329      1  \n",
      "76  -0.369498 -0.581329      2  \n",
      "43  -0.369498 -0.581329      1  \n",
      "24  -0.369498 -0.581329      1  \n",
      "3   -0.369498 -0.581329      1  \n",
      "182 -0.369498 -0.581329      6  \n",
      "49  -0.369498 -0.581329      1  \n",
      "149 -0.369498 -0.581329      3  \n",
      "131 -0.369498  0.425302      2  \n",
      "209  1.614258 -0.581329      7  \n",
      "30  -0.369498  0.827954      1  \n",
      "121 -0.369498  1.532596      2  \n",
      "175 -0.369498  2.237237      5  \n",
      "115 -0.369498 -0.581329      2  \n",
      "188 -0.369498 -0.581329      7  \n",
      "8   -0.369498 -0.581329      1  \n",
      "60  -0.369498 -0.581329      1  \n",
      "128  0.135798  1.129943      2  \n",
      "1   -0.369498 -0.581329      1  \n",
      "57  -0.369498 -0.581329      1  \n",
      "22  -0.369498 -0.581329      1  \n",
      "61   0.921815 -0.581329      1  \n",
      "63  -0.369498 -0.581329      1  \n",
      "7   -0.369498 -0.581329      1  \n",
      "210  2.606136 -0.581329      7  \n",
      "141 -0.201066  1.129943      2  \n",
      "86  -0.369498 -0.581329      2  \n",
      "96  -0.369498  0.928617      2  \n",
      "68  -0.369498  1.029280      1  \n",
      "50  -0.369498  1.029280      1  \n",
      "142 -0.257210  1.935248      2  \n",
      "157 -0.369498 -0.581329      3  \n",
      "156 -0.369498 -0.581329      3  \n",
      "139 -0.369498 -0.581329      2  \n",
      "146 -0.369498 -0.581329      3  \n",
      "101 -0.369498 -0.581329      2  \n",
      "20  -0.369498  1.331270      1  \n",
      "178 -0.369498 -0.581329      6  \n",
      "25  -0.369498 -0.581329      1  \n",
      "134 -0.369498 -0.581329      2  \n",
      "71  -0.369498  2.639890      2  \n",
      "129 -0.369498  1.230606      2  \n",
      "144 -0.369498  1.834585      2  \n",
      "192  0.379089  0.324638      7  \n",
      "79  -0.369498 -0.581329      2  \n",
      "133 -0.369498  0.928617      2  \n",
      "203  2.830713 -0.581329      7  \n",
      "137 -0.369498 -0.581329      2  \n",
      "72  -0.369498 -0.581329      2  \n",
      "140 -0.369498 -0.581329      2  \n",
      "37  -0.369498 -0.581329      1  , 'train_size': 100, 'test_size': 54, 'train_df':             1         2         3         4         5         6         7  \\\n",
      "108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
      "206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
      "106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
      "107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
      "51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "132 -0.074425  0.036211  0.931222 -0.534590 -0.226903  0.081385 -0.540582   \n",
      "193 -0.360995  1.544649 -1.763019  1.066374  0.428903 -0.705339 -0.292039   \n",
      "153 -0.693295  0.024784  0.538594 -0.456495  0.020571  0.094950 -0.429391   \n",
      "166  0.956010 -2.706404 -0.605443  0.207320  0.948598  0.081385  1.729012   \n",
      "160 -0.016501 -0.078065  0.497978  0.168272 -0.659982  0.054257  0.008830   \n",
      "\n",
      "            8         9  class  \n",
      "108 -0.369498  0.223975      2  \n",
      "206  2.213128 -0.581329      7  \n",
      "106  5.525627  2.237237      2  \n",
      "107 -0.369498  1.834585      2  \n",
      "51  -0.369498  0.525965      1  \n",
      "..        ...       ...    ...  \n",
      "132 -0.369498 -0.581329      2  \n",
      "193  2.606136  0.223975      7  \n",
      "153 -0.369498 -0.581329      3  \n",
      "166 -0.369498 -0.581329      5  \n",
      "160 -0.369498 -0.581329      3  \n",
      "\n",
      "[100 rows x 10 columns], 'test_df':             1         2         3         4         5         6         7  \\\n",
      "109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
      "80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
      "52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
      "26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
      "76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
      "43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
      "24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
      "3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
      "182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
      "49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
      "149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
      "131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
      "209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
      "30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
      "121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
      "175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
      "115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
      "188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
      "8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
      "60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
      "128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
      "1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
      "57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
      "22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
      "61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
      "63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
      "7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
      "210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
      "141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
      "86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
      "96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
      "68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
      "50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
      "142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
      "157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
      "156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
      "139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
      "146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
      "101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
      "20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
      "178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
      "25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
      "134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
      "71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
      "129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
      "144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
      "192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
      "79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
      "133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
      "203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
      "137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
      "72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
      "140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
      "37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
      "\n",
      "            8         9  class  \n",
      "109 -0.369498 -0.581329      2  \n",
      "80  -0.369498 -0.581329      2  \n",
      "52  -0.369498 -0.581329      1  \n",
      "26  -0.369498 -0.581329      1  \n",
      "76  -0.369498 -0.581329      2  \n",
      "43  -0.369498 -0.581329      1  \n",
      "24  -0.369498 -0.581329      1  \n",
      "3   -0.369498 -0.581329      1  \n",
      "182 -0.369498 -0.581329      6  \n",
      "49  -0.369498 -0.581329      1  \n",
      "149 -0.369498 -0.581329      3  \n",
      "131 -0.369498  0.425302      2  \n",
      "209  1.614258 -0.581329      7  \n",
      "30  -0.369498  0.827954      1  \n",
      "121 -0.369498  1.532596      2  \n",
      "175 -0.369498  2.237237      5  \n",
      "115 -0.369498 -0.581329      2  \n",
      "188 -0.369498 -0.581329      7  \n",
      "8   -0.369498 -0.581329      1  \n",
      "60  -0.369498 -0.581329      1  \n",
      "128  0.135798  1.129943      2  \n",
      "1   -0.369498 -0.581329      1  \n",
      "57  -0.369498 -0.581329      1  \n",
      "22  -0.369498 -0.581329      1  \n",
      "61   0.921815 -0.581329      1  \n",
      "63  -0.369498 -0.581329      1  \n",
      "7   -0.369498 -0.581329      1  \n",
      "210  2.606136 -0.581329      7  \n",
      "141 -0.201066  1.129943      2  \n",
      "86  -0.369498 -0.581329      2  \n",
      "96  -0.369498  0.928617      2  \n",
      "68  -0.369498  1.029280      1  \n",
      "50  -0.369498  1.029280      1  \n",
      "142 -0.257210  1.935248      2  \n",
      "157 -0.369498 -0.581329      3  \n",
      "156 -0.369498 -0.581329      3  \n",
      "139 -0.369498 -0.581329      2  \n",
      "146 -0.369498 -0.581329      3  \n",
      "101 -0.369498 -0.581329      2  \n",
      "20  -0.369498  1.331270      1  \n",
      "178 -0.369498 -0.581329      6  \n",
      "25  -0.369498 -0.581329      1  \n",
      "134 -0.369498 -0.581329      2  \n",
      "71  -0.369498  2.639890      2  \n",
      "129 -0.369498  1.230606      2  \n",
      "144 -0.369498  1.834585      2  \n",
      "192  0.379089  0.324638      7  \n",
      "79  -0.369498 -0.581329      2  \n",
      "133 -0.369498  0.928617      2  \n",
      "203  2.830713 -0.581329      7  \n",
      "137 -0.369498 -0.581329      2  \n",
      "72  -0.369498 -0.581329      2  \n",
      "140 -0.369498 -0.581329      2  \n",
      "37  -0.369498 -0.581329      1  , 'nclasses': 6, 'dt_len': 4.4, 'dt_acc': 0.6851851851851852, 'dt_clf': DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'lr_len': 5, 'lr_acc': 0.6481481481481481, 'lr_clf': LogisticRegression(C=0.20000000000000018, n_jobs=-1, penalty='l1',\n",
      "                   random_state=RandomState(MT19937) at 0x7F2CC67B7160,\n",
      "                   solver='liblinear')})])\n",
      "--- glass\n",
      "--- {'FOLDER_NAME': 'glass', 'SEPARATE_FILES': False, 'TRAIN_FILENAME': None, 'TEST_FILENAME': None, 'COMBINED_FILENAME': 'glass.data', 'CLASS_INDEX': -1, 'HAS_INDEX': True, 'HEADER_ROW_NUMBER': None, 'initial_features': 9, '_NUM': 9, '_CAT': 0, 'features': 9, 'initial_train_size': 160, 'initial_test_size': 54, 'original_train_df':             1         2         3         4         5         6         7  \\\n",
      "108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
      "206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
      "106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
      "107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
      "51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "201 -0.562204 -1.655068 -1.763019 -0.515066  3.101621  2.956999 -0.030414   \n",
      "64   1.020031  0.093349  0.768756 -1.081261 -0.820840 -0.461183  0.414348   \n",
      "15  -0.232953 -0.672298  0.633367 -0.436971  0.701124  0.081385 -0.383607   \n",
      "41  -0.251245 -0.786573  0.552133 -0.495542  0.651630  0.094950 -0.220092   \n",
      "176  0.206049  0.687582 -0.145120  0.207320 -0.375387 -0.705339  0.388186   \n",
      "\n",
      "            8         9  class  \n",
      "108 -0.369498  0.223975      2  \n",
      "206  2.213128 -0.581329      7  \n",
      "106  5.525627  2.237237      2  \n",
      "107 -0.369498  1.834585      2  \n",
      "51  -0.369498  0.525965      1  \n",
      "..        ...       ...    ...  \n",
      "201 -0.369498 -0.581329      7  \n",
      "64  -0.369498  0.123312      1  \n",
      "15  -0.369498 -0.581329      1  \n",
      "41  -0.369498 -0.581329      1  \n",
      "176 -0.369498 -0.581329      6  \n",
      "\n",
      "[160 rows x 10 columns], 'original_test_df':             1         2         3         4         5         6         7  \\\n",
      "109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
      "80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
      "52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
      "26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
      "76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
      "43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
      "24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
      "3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
      "182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
      "49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
      "149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
      "131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
      "209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
      "30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
      "121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
      "175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
      "115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
      "188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
      "8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
      "60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
      "128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
      "1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
      "57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
      "22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
      "61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
      "63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
      "7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
      "210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
      "141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
      "86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
      "96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
      "68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
      "50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
      "142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
      "157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
      "156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
      "139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
      "146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
      "101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
      "20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
      "178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
      "25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
      "134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
      "71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
      "129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
      "144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
      "192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
      "79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
      "133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
      "203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
      "137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
      "72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
      "140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
      "37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
      "\n",
      "            8         9  class  \n",
      "109 -0.369498 -0.581329      2  \n",
      "80  -0.369498 -0.581329      2  \n",
      "52  -0.369498 -0.581329      1  \n",
      "26  -0.369498 -0.581329      1  \n",
      "76  -0.369498 -0.581329      2  \n",
      "43  -0.369498 -0.581329      1  \n",
      "24  -0.369498 -0.581329      1  \n",
      "3   -0.369498 -0.581329      1  \n",
      "182 -0.369498 -0.581329      6  \n",
      "49  -0.369498 -0.581329      1  \n",
      "149 -0.369498 -0.581329      3  \n",
      "131 -0.369498  0.425302      2  \n",
      "209  1.614258 -0.581329      7  \n",
      "30  -0.369498  0.827954      1  \n",
      "121 -0.369498  1.532596      2  \n",
      "175 -0.369498  2.237237      5  \n",
      "115 -0.369498 -0.581329      2  \n",
      "188 -0.369498 -0.581329      7  \n",
      "8   -0.369498 -0.581329      1  \n",
      "60  -0.369498 -0.581329      1  \n",
      "128  0.135798  1.129943      2  \n",
      "1   -0.369498 -0.581329      1  \n",
      "57  -0.369498 -0.581329      1  \n",
      "22  -0.369498 -0.581329      1  \n",
      "61   0.921815 -0.581329      1  \n",
      "63  -0.369498 -0.581329      1  \n",
      "7   -0.369498 -0.581329      1  \n",
      "210  2.606136 -0.581329      7  \n",
      "141 -0.201066  1.129943      2  \n",
      "86  -0.369498 -0.581329      2  \n",
      "96  -0.369498  0.928617      2  \n",
      "68  -0.369498  1.029280      1  \n",
      "50  -0.369498  1.029280      1  \n",
      "142 -0.257210  1.935248      2  \n",
      "157 -0.369498 -0.581329      3  \n",
      "156 -0.369498 -0.581329      3  \n",
      "139 -0.369498 -0.581329      2  \n",
      "146 -0.369498 -0.581329      3  \n",
      "101 -0.369498 -0.581329      2  \n",
      "20  -0.369498  1.331270      1  \n",
      "178 -0.369498 -0.581329      6  \n",
      "25  -0.369498 -0.581329      1  \n",
      "134 -0.369498 -0.581329      2  \n",
      "71  -0.369498  2.639890      2  \n",
      "129 -0.369498  1.230606      2  \n",
      "144 -0.369498  1.834585      2  \n",
      "192  0.379089  0.324638      7  \n",
      "79  -0.369498 -0.581329      2  \n",
      "133 -0.369498  0.928617      2  \n",
      "203  2.830713 -0.581329      7  \n",
      "137 -0.369498 -0.581329      2  \n",
      "72  -0.369498 -0.581329      2  \n",
      "140 -0.369498 -0.581329      2  \n",
      "37  -0.369498 -0.581329      1  , 'train_size': 100, 'test_size': 54, 'train_df':             1         2         3         4         5         6         7  \\\n",
      "108  1.172462  1.178967 -1.763019 -0.886022 -0.004176 -0.569697  1.663606   \n",
      "206 -0.586593  1.761773 -1.763019  0.812563  0.540266 -0.705339 -0.200470   \n",
      "106  3.925370 -3.049231 -1.763019  1.261614 -3.543053  0.081385  2.827836   \n",
      "107  4.742401 -1.255103 -1.763019 -0.886022 -3.109974 -0.542569  4.718074   \n",
      "51   0.270070 -0.226623  0.491208 -0.339351 -0.387761  0.108514  0.106939   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "132 -0.074425  0.036211  0.931222 -0.534590 -0.226903  0.081385 -0.540582   \n",
      "193 -0.360995  1.544649 -1.763019  1.066374  0.428903 -0.705339 -0.292039   \n",
      "153 -0.693295  0.024784  0.538594 -0.456495  0.020571  0.094950 -0.429391   \n",
      "166  0.956010 -2.706404 -0.605443  0.207320  0.948598  0.081385  1.729012   \n",
      "160 -0.016501 -0.078065  0.497978  0.168272 -0.659982  0.054257  0.008830   \n",
      "\n",
      "            8         9  class  \n",
      "108 -0.369498  0.223975      2  \n",
      "206  2.213128 -0.581329      7  \n",
      "106  5.525627  2.237237      2  \n",
      "107 -0.369498  1.834585      2  \n",
      "51  -0.369498  0.525965      1  \n",
      "..        ...       ...    ...  \n",
      "132 -0.369498 -0.581329      2  \n",
      "193  2.606136  0.223975      7  \n",
      "153 -0.369498 -0.581329      3  \n",
      "166 -0.369498 -0.581329      5  \n",
      "160 -0.369498 -0.581329      3  \n",
      "\n",
      "[100 rows x 10 columns], 'test_df':             1         2         3         4         5         6         7  \\\n",
      "109 -0.059181  0.367610 -1.763019 -1.745076  2.198342 -0.705339  1.316953   \n",
      "80  -0.748170 -0.615160  0.619828  1.300662 -0.016550  0.230592 -0.658313   \n",
      "52  -0.089668  0.036211  0.179814 -0.515066  0.206177  0.040693  0.034992   \n",
      "26  -0.135397 -0.215195  0.592750 -0.085539 -0.041297  0.094950 -0.357445   \n",
      "76  -0.586593  0.047639  0.680753  0.168272 -0.350640  0.189899 -0.619069   \n",
      "43   1.135879  0.379038  0.836450 -1.432692 -1.130182 -0.474748  0.499376   \n",
      "24  -0.357947 -0.020927  0.606289 -0.593162  0.218550 -0.027128 -0.357445   \n",
      "3   -0.217710 -0.215195  0.734908 -0.319827 -0.078418  0.067821 -0.494798   \n",
      "182  0.239584  0.858996 -1.763019  1.242090  0.082440 -0.705339  1.245006   \n",
      "49   0.184708  0.207625  0.504747 -0.436971 -0.734224  0.094950 -0.043495   \n",
      "149 -0.592691 -1.415089  0.619828 -0.202683  0.268045  0.067821 -0.292039   \n",
      "131  2.367523  0.344755 -1.763019 -0.183159 -1.773614 -0.447619  2.919404   \n",
      "209 -0.653663  0.847568 -1.763019  2.784483 -0.078418 -0.596826  0.133102   \n",
      "30  -0.211613 -0.855139  0.646906 -0.300303  0.503145  0.122078 -0.187389   \n",
      "121 -0.531718 -0.535167  0.633367  0.324464  0.354661  0.162771 -0.619069   \n",
      "175  0.858454 -0.489457 -1.539627  0.109700  0.886730 -0.529004  1.500090   \n",
      "115  0.026180  0.013356  0.870297 -0.241731 -0.363013 -0.013564 -0.455554   \n",
      "188  1.248678  1.670352 -0.273740  1.183518 -2.986237  0.325541  0.512457   \n",
      "8    0.245681  0.733292  0.660444 -0.163635 -0.734224  0.054257 -0.442473   \n",
      "60   0.206049  0.230480  0.687522 -0.671258 -0.041297 -0.515440 -0.141604   \n",
      "128  0.702974  0.173342 -0.348204  0.422084 -0.610487  0.013564  0.388186   \n",
      "1   -0.232953  0.561879  0.673983 -0.183159  0.070066 -0.054257 -0.749882   \n",
      "57  -0.040890 -0.603732  0.592750 -0.319827  0.342287  0.108514 -0.357445   \n",
      "22  -0.309169 -0.706580  0.687522 -0.319827  0.144308  0.094950 -0.180848   \n",
      "61   0.425550  0.470458  0.660444 -0.261255 -1.179677 -0.542569 -0.200470   \n",
      "63   1.187706  0.881851  0.816142 -1.315549 -1.637504 -0.705339  0.466673   \n",
      "7   -0.248196 -0.283761  0.680753 -0.788402  0.701124  0.067821 -0.481716   \n",
      "210 -0.464648  1.738918 -1.763019  1.046850  0.478398 -0.705339 -0.377067   \n",
      "141  0.041423 -0.226623  0.694292 -0.749354  0.193803  0.067821 -0.370526   \n",
      "86  -0.818289 -0.180913  0.599519  0.031604  0.713498 -0.189899 -0.619069   \n",
      "96   0.010937 -0.432319  0.687522 -0.768878 -0.412508  0.162771  0.100398   \n",
      "68   0.959059 -0.318043  0.660444 -1.081261 -0.585740 -0.393362  0.551701   \n",
      "50   1.471228  0.367610  0.755217 -1.842696 -1.142556 -0.583261  0.708676   \n",
      "142 -0.534767 -0.626588  0.613058 -0.026967  0.416530  0.217027 -0.488257   \n",
      "157  0.864552  0.721865  0.782295 -1.706028 -1.093061 -0.556133  0.440511   \n",
      "156 -0.556107  0.013356  0.531825 -0.339351 -0.041297  0.000000 -0.213551   \n",
      "139 -0.498183 -0.603732  0.646906  0.363512  0.577388  0.176335 -0.645232   \n",
      "146 -0.208564  0.287617  0.714600 -0.671258  0.119561 -0.556133 -0.246254   \n",
      "101 -0.327460 -1.197966  0.078272  0.343988  0.243298  0.244156  0.165805   \n",
      "20  -0.266488 -0.660870  0.640136  0.070652  0.094813  0.027128 -0.298579   \n",
      "178 -0.025647  1.213250 -0.246662  0.324464 -0.363013 -0.705339  0.185426   \n",
      "25  -0.223807 -0.478029  0.633367 -0.476019  0.404156  0.176335 -0.292039   \n",
      "134 -0.080522 -0.078065  0.843220 -0.397923  0.131935  0.000000 -0.560204   \n",
      "71   0.032277  0.276190  0.856759 -0.358875 -0.882708  0.027128 -0.429391   \n",
      "129  0.556641  0.664727 -0.849143  0.343988 -1.130182 -0.176335  1.035707   \n",
      "144 -0.540864 -0.466602  0.389666 -0.436971  0.367035  0.081385 -0.108901   \n",
      "192 -0.653663  0.916133 -1.763019  2.608767  0.973346 -0.651082  0.041533   \n",
      "79  -0.754268 -0.660870  0.619828  0.871135  0.230924  0.230592 -0.658313   \n",
      "133 -0.114057  0.356183  0.897375  0.168272 -1.068314  0.027128 -0.501338   \n",
      "203 -0.546961  1.601787 -1.763019  1.046850  0.540266 -0.705339 -0.455554   \n",
      "137 -0.385384 -0.580877  0.687522  0.226844  0.354661  0.122078 -0.566744   \n",
      "72  -0.745122 -0.352326  0.667214  0.129224  0.527893  0.203463 -0.749882   \n",
      "140 -0.449405 -0.078065  0.633367  0.304940 -0.165034  0.217027 -0.566744   \n",
      "37  -0.123202 -0.752291  0.592750 -0.202683  0.354661  0.162771 -0.193929   \n",
      "\n",
      "            8         9  class  \n",
      "109 -0.369498 -0.581329      2  \n",
      "80  -0.369498 -0.581329      2  \n",
      "52  -0.369498 -0.581329      1  \n",
      "26  -0.369498 -0.581329      1  \n",
      "76  -0.369498 -0.581329      2  \n",
      "43  -0.369498 -0.581329      1  \n",
      "24  -0.369498 -0.581329      1  \n",
      "3   -0.369498 -0.581329      1  \n",
      "182 -0.369498 -0.581329      6  \n",
      "49  -0.369498 -0.581329      1  \n",
      "149 -0.369498 -0.581329      3  \n",
      "131 -0.369498  0.425302      2  \n",
      "209  1.614258 -0.581329      7  \n",
      "30  -0.369498  0.827954      1  \n",
      "121 -0.369498  1.532596      2  \n",
      "175 -0.369498  2.237237      5  \n",
      "115 -0.369498 -0.581329      2  \n",
      "188 -0.369498 -0.581329      7  \n",
      "8   -0.369498 -0.581329      1  \n",
      "60  -0.369498 -0.581329      1  \n",
      "128  0.135798  1.129943      2  \n",
      "1   -0.369498 -0.581329      1  \n",
      "57  -0.369498 -0.581329      1  \n",
      "22  -0.369498 -0.581329      1  \n",
      "61   0.921815 -0.581329      1  \n",
      "63  -0.369498 -0.581329      1  \n",
      "7   -0.369498 -0.581329      1  \n",
      "210  2.606136 -0.581329      7  \n",
      "141 -0.201066  1.129943      2  \n",
      "86  -0.369498 -0.581329      2  \n",
      "96  -0.369498  0.928617      2  \n",
      "68  -0.369498  1.029280      1  \n",
      "50  -0.369498  1.029280      1  \n",
      "142 -0.257210  1.935248      2  \n",
      "157 -0.369498 -0.581329      3  \n",
      "156 -0.369498 -0.581329      3  \n",
      "139 -0.369498 -0.581329      2  \n",
      "146 -0.369498 -0.581329      3  \n",
      "101 -0.369498 -0.581329      2  \n",
      "20  -0.369498  1.331270      1  \n",
      "178 -0.369498 -0.581329      6  \n",
      "25  -0.369498 -0.581329      1  \n",
      "134 -0.369498 -0.581329      2  \n",
      "71  -0.369498  2.639890      2  \n",
      "129 -0.369498  1.230606      2  \n",
      "144 -0.369498  1.834585      2  \n",
      "192  0.379089  0.324638      7  \n",
      "79  -0.369498 -0.581329      2  \n",
      "133 -0.369498  0.928617      2  \n",
      "203  2.830713 -0.581329      7  \n",
      "137 -0.369498 -0.581329      2  \n",
      "72  -0.369498 -0.581329      2  \n",
      "140 -0.369498 -0.581329      2  \n",
      "37  -0.369498 -0.581329      1  , 'nclasses': 6, 'dt_len': 4.4, 'dt_acc': 0.6851851851851852, 'dt_clf': DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'lr_len': 5, 'lr_acc': 0.6481481481481481, 'lr_clf': LogisticRegression(C=0.20000000000000018, n_jobs=-1, penalty='l1',\n",
      "                   random_state=RandomState(MT19937) at 0x7F2CC67B7160,\n",
      "                   solver='liblinear')}\n",
      "['/cshome/motalleb/DM/UCI/archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data']\n",
      "           1      2     3     4      5     6      7     8     9  class\n",
      "25   1.51764  12.98  3.54  1.21  73.00  0.65   8.53  0.00  0.00      1\n",
      "208  1.51640  14.37  0.00  2.74  72.85  0.00   9.45  0.54  0.00      7\n",
      "122  1.51687  13.23  3.54  1.48  72.84  0.56   8.10  0.00  0.00      2\n",
      "50   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.00  0.16      1\n",
      "174  1.52058  12.85  1.61  2.17  72.18  0.76   9.70  0.24  0.51      5\n",
      "..       ...    ...   ...   ...    ...   ...    ...   ...   ...    ...\n",
      "37   1.51797  12.74  3.48  1.35  72.96  0.64   8.68  0.00  0.00      1\n",
      "182  1.51916  14.15  0.00  2.09  72.74  0.00  10.88  0.00  0.00      6\n",
      "114  1.51847  13.10  3.97  1.19  72.44  0.60   8.43  0.00  0.00      2\n",
      "53   1.51837  13.14  2.84  1.28  72.85  0.55   9.07  0.00  0.00      1\n",
      "185  1.51131  13.69  3.20  1.81  72.81  1.76   5.43  1.19  0.00      7\n",
      "\n",
      "[160 rows x 10 columns]\n",
      "{'class': 'class'}\n",
      "25     1\n",
      "208    7\n",
      "122    2\n",
      "50     1\n",
      "174    5\n",
      "      ..\n",
      "37     1\n",
      "182    6\n",
      "114    2\n",
      "53     1\n",
      "185    7\n",
      "Name: class, Length: 160, dtype: int64\n",
      "           1      2     3     4      5     6      7     8     9\n",
      "25   1.51764  12.98  3.54  1.21  73.00  0.65   8.53  0.00  0.00\n",
      "208  1.51640  14.37  0.00  2.74  72.85  0.00   9.45  0.54  0.00\n",
      "122  1.51687  13.23  3.54  1.48  72.84  0.56   8.10  0.00  0.00\n",
      "50   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.00  0.16\n",
      "174  1.52058  12.85  1.61  2.17  72.18  0.76   9.70  0.24  0.51\n",
      "..       ...    ...   ...   ...    ...   ...    ...   ...   ...\n",
      "37   1.51797  12.74  3.48  1.35  72.96  0.64   8.68  0.00  0.00\n",
      "182  1.51916  14.15  0.00  2.09  72.74  0.00  10.88  0.00  0.00\n",
      "114  1.51847  13.10  3.97  1.19  72.44  0.60   8.43  0.00  0.00\n",
      "53   1.51837  13.14  2.84  1.28  72.85  0.55   9.07  0.00  0.00\n",
      "185  1.51131  13.69  3.20  1.81  72.81  1.76   5.43  1.19  0.00\n",
      "\n",
      "[160 rows x 9 columns]\n",
      "           1      2     3     4      5     6      7     8     9\n",
      "25   1.51764  12.98  3.54  1.21  73.00  0.65   8.53  0.00  0.00\n",
      "208  1.51640  14.37  0.00  2.74  72.85  0.00   9.45  0.54  0.00\n",
      "122  1.51687  13.23  3.54  1.48  72.84  0.56   8.10  0.00  0.00\n",
      "50   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.00  0.16\n",
      "174  1.52058  12.85  1.61  2.17  72.18  0.76   9.70  0.24  0.51\n",
      "..       ...    ...   ...   ...    ...   ...    ...   ...   ...\n",
      "37   1.51797  12.74  3.48  1.35  72.96  0.64   8.68  0.00  0.00\n",
      "182  1.51916  14.15  0.00  2.09  72.74  0.00  10.88  0.00  0.00\n",
      "114  1.51847  13.10  3.97  1.19  72.44  0.60   8.43  0.00  0.00\n",
      "53   1.51837  13.14  2.84  1.28  72.85  0.55   9.07  0.00  0.00\n",
      "185  1.51131  13.69  3.20  1.81  72.81  1.76   5.43  1.19  0.00\n",
      "\n",
      "[160 rows x 9 columns]\n",
      "           1      2     3     4      5     6      7     8     9\n",
      "143  1.51709  13.00  3.47  1.79  72.72  0.66   8.18  0.00  0.00\n",
      "168  1.51666  12.86  0.00  1.83  73.88  0.97  10.17  0.00  0.00\n",
      "71   1.51848  13.64  3.87  1.27  71.96  0.54   8.32  0.00  0.32\n",
      "54   1.51778  13.21  2.81  1.29  72.98  0.51   9.02  0.00  0.09\n",
      "88   1.51618  13.01  3.50  1.48  72.89  0.60   8.12  0.00  0.00\n",
      "131  1.52614  13.70  0.00  1.36  71.24  0.19  13.44  0.00  0.10\n",
      "124  1.52177  13.20  3.68  1.15  72.75  0.54   8.52  0.00  0.00\n",
      "21   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.00  0.00\n",
      "14   1.51763  12.61  3.59  1.31  73.29  0.58   8.50  0.00  0.00\n",
      "151  1.52127  14.32  3.90  0.83  71.50  0.00   9.49  0.00  0.00\n",
      "196  1.51556  13.87  0.00  2.54  73.23  0.14   9.41  0.81  0.01\n",
      "123  1.51707  13.48  3.48  1.71  72.52  0.62   7.99  0.00  0.00\n",
      "138  1.51674  12.79  3.52  1.54  73.36  0.66   7.90  0.00  0.00\n",
      "111  1.52739  11.02  0.00  0.75  73.08  0.00  14.96  0.00  0.00\n",
      "51   1.51926  13.20  3.33  1.28  72.36  0.60   9.14  0.00  0.11\n",
      "112  1.52777  12.64  0.00  0.67  72.02  0.06  14.40  0.00  0.00\n",
      "9    1.51755  13.00  3.60  1.36  72.99  0.57   8.40  0.00  0.11\n",
      "19   1.51735  13.02  3.54  1.69  72.73  0.54   8.44  0.00  0.07\n",
      "16   1.51784  12.68  3.67  1.16  73.11  0.61   8.70  0.00  0.00\n",
      "86   1.51569  13.24  3.49  1.47  73.25  0.38   8.03  0.00  0.00\n",
      "0    1.52101  13.64  4.49  1.10  71.78  0.06   8.75  0.00  0.00\n",
      "105  1.52475  11.45  0.00  1.88  72.19  0.81  13.24  0.00  0.34\n",
      "44   1.51786  12.73  3.43  1.19  72.95  0.62   8.76  0.00  0.30\n",
      "178  1.51829  14.46  2.24  1.62  72.38  0.00   9.26  0.00  0.00\n",
      "48   1.52223  13.21  3.77  0.79  71.99  0.13  10.02  0.00  0.00\n",
      "153  1.51610  13.42  3.40  1.22  72.69  0.59   8.32  0.00  0.00\n",
      "70   1.51574  14.86  3.67  1.74  71.87  0.16   7.36  0.00  0.12\n",
      "169  1.51994  13.27  0.00  1.76  73.03  0.47  11.32  0.00  0.00\n",
      "38   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.00  0.00\n",
      "150  1.51665  13.14  3.45  1.76  72.48  0.60   8.38  0.00  0.17\n",
      "156  1.51655  13.41  3.39  1.28  72.64  0.52   8.65  0.00  0.00\n",
      "163  1.51514  14.01  2.68  3.50  69.89  1.68   5.87  2.20  0.00\n",
      "167  1.51969  12.64  0.00  1.65  73.75  0.38  11.53  0.00  0.00\n",
      "197  1.51727  14.70  0.00  2.34  73.28  0.00   8.95  0.66  0.00\n",
      "188  1.52247  14.86  2.20  2.06  70.26  0.76   9.76  0.00  0.00\n",
      "64   1.52172  13.48  3.74  0.90  72.01  0.18   9.61  0.00  0.07\n",
      "68   1.52152  13.12  3.58  0.90  72.20  0.23   9.82  0.00  0.16\n",
      "145  1.51839  12.85  3.67  1.24  72.57  0.62   8.68  0.00  0.35\n",
      "158  1.51776  13.53  3.41  1.52  72.04  0.58   8.79  0.00  0.00\n",
      "194  1.51683  14.56  0.00  1.98  73.29  0.00   8.52  1.57  0.07\n",
      "42   1.51779  13.21  3.39  1.33  72.76  0.59   8.59  0.00  0.00\n",
      "160  1.51832  13.33  3.34  1.54  72.14  0.56   8.99  0.00  0.00\n",
      "87   1.51645  13.40  3.49  1.52  72.65  0.67   8.08  0.00  0.10\n",
      "113  1.51892  13.46  3.83  1.26  72.55  0.57   8.21  0.00  0.14\n",
      "63   1.52227  14.17  3.81  0.78  71.35  0.00   9.69  0.00  0.00\n",
      "79   1.51590  12.82  3.52  1.90  72.86  0.69   7.97  0.00  0.00\n",
      "133  1.51800  13.71  3.93  1.54  71.81  0.54   8.21  0.00  0.15\n",
      "175  1.52119  12.97  0.33  1.51  73.39  0.13  11.27  0.00  0.28\n",
      "166  1.52151  11.03  1.71  1.56  73.44  0.58  11.62  0.00  0.00\n",
      "94   1.51629  12.71  3.33  1.49  73.28  0.67   8.24  0.00  0.00\n",
      "24   1.51720  13.38  3.50  1.15  72.85  0.50   8.43  0.00  0.00\n",
      "195  1.51545  14.14  0.00  2.68  73.39  0.08   9.07  0.61  0.05\n",
      "147  1.51610  13.33  3.53  1.34  72.67  0.56   8.33  0.00  0.00\n",
      "32   1.51775  12.85  3.48  1.23  72.97  0.61   8.56  0.09  0.22\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "            1         2         3         4         5         6         7  \\\n",
      "25  -0.181553 -0.559513  0.602956 -0.479225  0.404394  0.185650 -0.246753   \n",
      "208 -0.592087  1.141358 -1.885491  2.682035  0.209310 -0.704922  0.459353   \n",
      "122 -0.436482 -0.253601  0.602956  0.078644  0.196304  0.062340 -0.576781   \n",
      "50   1.659226  0.345986  0.729488 -1.925554 -1.221312 -0.581612  0.927532   \n",
      "174  0.791808 -0.718587 -0.753739  1.504311 -0.662069  0.336362  0.651229   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "37  -0.072299 -0.853189  0.560779 -0.189960  0.352372  0.171949 -0.131627   \n",
      "182  0.321681  0.872155 -1.885491  1.339016  0.066248 -0.704922  1.556887   \n",
      "114  0.093239 -0.412675  0.905225 -0.520549 -0.323922  0.117144 -0.323504   \n",
      "53   0.060132 -0.363729  0.110891 -0.334593  0.209310  0.048639  0.167700   \n",
      "185 -2.277261  0.309277  0.363953  0.760485  0.157287  1.706473 -2.626024   \n",
      "\n",
      "            8         9  class  \n",
      "25  -0.374685 -0.574827      1  \n",
      "208  0.652370 -0.574827      7  \n",
      "122 -0.374685 -0.574827      2  \n",
      "50  -0.374685  1.084201      1  \n",
      "174  0.081784  4.713325      5  \n",
      "..        ...       ...    ...  \n",
      "37  -0.374685 -0.574827      1  \n",
      "182 -0.374685 -0.574827      6  \n",
      "114 -0.374685 -0.574827      2  \n",
      "53  -0.374685 -0.574827      1  \n",
      "185  1.888639 -0.574827      7  \n",
      "\n",
      "[160 rows x 10 columns]\n",
      "            1         2         3         4         5         6         7  \\\n",
      "143 -0.363645 -0.535040  0.553750  0.719161  0.040236  0.199351 -0.515381   \n",
      "168 -0.506007 -0.706351 -1.885491  0.801808  1.548891  0.624085  1.011957   \n",
      "71   0.096550  0.248095  0.834930 -0.355254 -0.948193  0.034938 -0.407930   \n",
      "54  -0.135203 -0.278074  0.089802 -0.313931  0.378383 -0.006165  0.129325   \n",
      "88  -0.664924 -0.522804  0.574838  0.078644  0.261332  0.117144 -0.561431   \n",
      "131  2.632587  0.321514 -1.885491 -0.169298 -1.884600 -0.444601  3.521704   \n",
      "124  1.185788 -0.290310  0.701370 -0.603196  0.079253  0.034938 -0.254428   \n",
      "21   0.487219  1.630817  0.750576 -2.380114 -0.870159 -0.663819  0.113975   \n",
      "14  -0.184864 -1.012263  0.638104 -0.272607  0.781558  0.089742 -0.269779   \n",
      "151  1.020250  1.080175  0.856019 -1.264375 -1.546453 -0.704922  0.490053   \n",
      "196 -0.870190  0.529534 -1.885491  2.268799  0.703524 -0.513107  0.428652   \n",
      "123 -0.370266  0.052311  0.560779  0.553866 -0.219877  0.144547 -0.661207   \n",
      "138 -0.479521 -0.792006  0.588897  0.202615  0.872598  0.199351 -0.730283   \n",
      "111  3.046432 -2.957863 -1.885491 -1.429670  0.508440 -0.704922  4.688314   \n",
      "51   0.354789 -0.290310  0.455337 -0.334593 -0.427967  0.117144  0.221426   \n",
      "112  3.172240 -0.975553 -1.885491 -1.594965 -0.870159 -0.622715  4.258510   \n",
      "9   -0.211350 -0.535040  0.645133 -0.169298  0.391389  0.076041 -0.346529   \n",
      "19  -0.277565 -0.510567  0.602956  0.512543  0.053242  0.034938 -0.315829   \n",
      "16  -0.115338 -0.926607  0.694340 -0.582535  0.547456  0.130846 -0.116277   \n",
      "86  -0.827151 -0.241365  0.567809  0.057982  0.729536 -0.184280 -0.630507   \n",
      "0    0.934171  0.248095  1.270760 -0.706506 -1.182295 -0.622715 -0.077902   \n",
      "105  2.172393 -2.431694 -1.885491  0.905118 -0.649063  0.404868  3.368203   \n",
      "44  -0.108717 -0.865425  0.525632 -0.520549  0.339366  0.144547 -0.070227   \n",
      "178  0.033646  1.251486 -0.310880  0.367910 -0.401956 -0.704922  0.313526   \n",
      "48   1.338083 -0.278074  0.764635 -1.347023 -0.909176 -0.526808  0.896831   \n",
      "153 -0.691410 -0.021108  0.504543 -0.458564  0.001219  0.103443 -0.407930   \n",
      "70  -0.810597  1.740945  0.694340  0.615852 -1.065244 -0.485704 -1.144736   \n",
      "169  0.579920 -0.204655 -1.885491  0.657175  0.443411 -0.060970  1.894590   \n",
      "38   1.304975  0.945574  0.799783 -2.008201 -1.195300 -0.554210  0.551453   \n",
      "150 -0.509318 -0.363729  0.539691  0.657175 -0.271899  0.117144 -0.361879   \n",
      "156 -0.542426 -0.033344  0.497514 -0.334593 -0.063809  0.007536 -0.154653   \n",
      "163 -1.009242  0.700844 -0.001582  4.252335 -3.640362  1.596864 -2.288321   \n",
      "167  0.497151 -0.975553 -1.885491  0.429895  1.379818 -0.184280  2.055766   \n",
      "197 -0.304051  1.545161 -1.885491  1.855562  0.768552 -0.704922  0.075599   \n",
      "188  1.417541  1.740945 -0.338998  1.277031 -3.159153  0.336362  0.697280   \n",
      "64   1.169234  0.052311  0.743547 -1.119742 -0.883165 -0.458302  0.582154   \n",
      "68   1.103019 -0.388202  0.631074 -1.119742 -0.636057 -0.389797  0.743330   \n",
      "145  0.066753 -0.718587  0.694340 -0.417240 -0.154848  0.144547 -0.131627   \n",
      "158 -0.141824  0.113493  0.511573  0.161291 -0.844148  0.089742 -0.047202   \n",
      "194 -0.449725  1.373851 -1.885491  1.111736  0.781558 -0.704922 -0.254428   \n",
      "42  -0.131892 -0.278074  0.497514 -0.231283  0.092259  0.103443 -0.200703   \n",
      "160  0.043578 -0.131236  0.462366  0.202615 -0.714091  0.062340  0.106300   \n",
      "87  -0.575533 -0.045581  0.567809  0.161291 -0.050803  0.213052 -0.592131   \n",
      "113  0.242223  0.027838  0.806812 -0.375916 -0.180860  0.076041 -0.492355   \n",
      "63   1.351326  0.896628  0.792753 -1.367684 -1.741537 -0.704922  0.643554   \n",
      "79  -0.757625 -0.755297  0.588897  0.946441  0.222315  0.240454 -0.676557   \n",
      "133 -0.062366  0.333750  0.877107  0.202615 -1.143278  0.034938 -0.492355   \n",
      "175  0.993764 -0.571750 -1.653517  0.140630  0.911615 -0.526808  1.856215   \n",
      "166  1.099708 -2.945627 -0.683444  0.243939  0.976643  0.089742  2.124842   \n",
      "94  -0.628505 -0.889898  0.455337  0.099306  0.768552  0.213052 -0.469330   \n",
      "24  -0.327227 -0.070054  0.574838 -0.603196  0.209310 -0.019867 -0.323504   \n",
      "195 -0.906609  0.859919 -1.885491  2.558064  0.911615 -0.595313  0.167700   \n",
      "147 -0.691410 -0.131236  0.595927 -0.210622 -0.024792  0.062340 -0.400255   \n",
      "32  -0.145135 -0.718587  0.560779 -0.437902  0.365377  0.130846 -0.223728   \n",
      "\n",
      "            8         9  class  \n",
      "143 -0.374685 -0.574827      2  \n",
      "168 -0.374685 -0.574827      5  \n",
      "71  -0.374685  2.743229      2  \n",
      "54  -0.374685  0.358376      1  \n",
      "88  -0.374685 -0.574827      2  \n",
      "131 -0.374685  0.462065      2  \n",
      "124 -0.374685 -0.574827      2  \n",
      "21  -0.374685 -0.574827      1  \n",
      "14  -0.374685 -0.574827      1  \n",
      "151 -0.374685 -0.574827      3  \n",
      "196  1.165897 -0.471138      7  \n",
      "123 -0.374685 -0.574827      2  \n",
      "138 -0.374685 -0.574827      2  \n",
      "111 -0.374685 -0.574827      2  \n",
      "51  -0.374685  0.565755      1  \n",
      "112 -0.374685 -0.574827      2  \n",
      "9   -0.374685  0.565755      1  \n",
      "19  -0.374685  0.150997      1  \n",
      "16  -0.374685 -0.574827      1  \n",
      "86  -0.374685 -0.574827      2  \n",
      "0   -0.374685 -0.574827      1  \n",
      "105 -0.374685  2.950608      2  \n",
      "44  -0.374685  2.535851      1  \n",
      "178 -0.374685 -0.574827      6  \n",
      "48  -0.374685 -0.574827      1  \n",
      "153 -0.374685 -0.574827      3  \n",
      "70  -0.374685  0.669444      2  \n",
      "169 -0.374685 -0.574827      5  \n",
      "38  -0.374685 -0.574827      1  \n",
      "150 -0.374685  1.187890      3  \n",
      "156 -0.374685 -0.574827      3  \n",
      "163  3.809612 -0.574827      5  \n",
      "167 -0.374685 -0.574827      5  \n",
      "197  0.880604 -0.574827      7  \n",
      "188 -0.374685 -0.574827      7  \n",
      "64  -0.374685  0.150997      1  \n",
      "68  -0.374685  1.084201      1  \n",
      "145 -0.374685  3.054297      2  \n",
      "158 -0.374685 -0.574827      3  \n",
      "194  2.611381  0.150997      7  \n",
      "42  -0.374685 -0.574827      1  \n",
      "160 -0.374685 -0.574827      3  \n",
      "87  -0.374685  0.462065      2  \n",
      "113 -0.374685  0.876822      2  \n",
      "63  -0.374685 -0.574827      1  \n",
      "79  -0.374685 -0.574827      2  \n",
      "133 -0.374685  0.980512      2  \n",
      "175 -0.374685  2.328472      5  \n",
      "166 -0.374685 -0.574827      5  \n",
      "94  -0.374685 -0.574827      2  \n",
      "24  -0.374685 -0.574827      1  \n",
      "195  0.785507 -0.056381      7  \n",
      "147 -0.374685 -0.574827      3  \n",
      "32  -0.203509  1.706337      1  \n",
      "dict_items([('glass', {'FOLDER_NAME': 'glass', 'SEPARATE_FILES': False, 'TRAIN_FILENAME': None, 'TEST_FILENAME': None, 'COMBINED_FILENAME': 'glass.data', 'CLASS_INDEX': -1, 'HAS_INDEX': True, 'HEADER_ROW_NUMBER': None, 'initial_features': 9, '_NUM': 9, '_CAT': 0, 'features': 9, 'initial_train_size': 160, 'initial_test_size': 54, 'original_train_df':             1         2         3         4         5         6         7  \\\n",
      "25  -0.181553 -0.559513  0.602956 -0.479225  0.404394  0.185650 -0.246753   \n",
      "208 -0.592087  1.141358 -1.885491  2.682035  0.209310 -0.704922  0.459353   \n",
      "122 -0.436482 -0.253601  0.602956  0.078644  0.196304  0.062340 -0.576781   \n",
      "50   1.659226  0.345986  0.729488 -1.925554 -1.221312 -0.581612  0.927532   \n",
      "174  0.791808 -0.718587 -0.753739  1.504311 -0.662069  0.336362  0.651229   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "37  -0.072299 -0.853189  0.560779 -0.189960  0.352372  0.171949 -0.131627   \n",
      "182  0.321681  0.872155 -1.885491  1.339016  0.066248 -0.704922  1.556887   \n",
      "114  0.093239 -0.412675  0.905225 -0.520549 -0.323922  0.117144 -0.323504   \n",
      "53   0.060132 -0.363729  0.110891 -0.334593  0.209310  0.048639  0.167700   \n",
      "185 -2.277261  0.309277  0.363953  0.760485  0.157287  1.706473 -2.626024   \n",
      "\n",
      "            8         9  class  \n",
      "25  -0.374685 -0.574827      1  \n",
      "208  0.652370 -0.574827      7  \n",
      "122 -0.374685 -0.574827      2  \n",
      "50  -0.374685  1.084201      1  \n",
      "174  0.081784  4.713325      5  \n",
      "..        ...       ...    ...  \n",
      "37  -0.374685 -0.574827      1  \n",
      "182 -0.374685 -0.574827      6  \n",
      "114 -0.374685 -0.574827      2  \n",
      "53  -0.374685 -0.574827      1  \n",
      "185  1.888639 -0.574827      7  \n",
      "\n",
      "[160 rows x 10 columns], 'original_test_df':             1         2         3         4         5         6         7  \\\n",
      "143 -0.363645 -0.535040  0.553750  0.719161  0.040236  0.199351 -0.515381   \n",
      "168 -0.506007 -0.706351 -1.885491  0.801808  1.548891  0.624085  1.011957   \n",
      "71   0.096550  0.248095  0.834930 -0.355254 -0.948193  0.034938 -0.407930   \n",
      "54  -0.135203 -0.278074  0.089802 -0.313931  0.378383 -0.006165  0.129325   \n",
      "88  -0.664924 -0.522804  0.574838  0.078644  0.261332  0.117144 -0.561431   \n",
      "131  2.632587  0.321514 -1.885491 -0.169298 -1.884600 -0.444601  3.521704   \n",
      "124  1.185788 -0.290310  0.701370 -0.603196  0.079253  0.034938 -0.254428   \n",
      "21   0.487219  1.630817  0.750576 -2.380114 -0.870159 -0.663819  0.113975   \n",
      "14  -0.184864 -1.012263  0.638104 -0.272607  0.781558  0.089742 -0.269779   \n",
      "151  1.020250  1.080175  0.856019 -1.264375 -1.546453 -0.704922  0.490053   \n",
      "196 -0.870190  0.529534 -1.885491  2.268799  0.703524 -0.513107  0.428652   \n",
      "123 -0.370266  0.052311  0.560779  0.553866 -0.219877  0.144547 -0.661207   \n",
      "138 -0.479521 -0.792006  0.588897  0.202615  0.872598  0.199351 -0.730283   \n",
      "111  3.046432 -2.957863 -1.885491 -1.429670  0.508440 -0.704922  4.688314   \n",
      "51   0.354789 -0.290310  0.455337 -0.334593 -0.427967  0.117144  0.221426   \n",
      "112  3.172240 -0.975553 -1.885491 -1.594965 -0.870159 -0.622715  4.258510   \n",
      "9   -0.211350 -0.535040  0.645133 -0.169298  0.391389  0.076041 -0.346529   \n",
      "19  -0.277565 -0.510567  0.602956  0.512543  0.053242  0.034938 -0.315829   \n",
      "16  -0.115338 -0.926607  0.694340 -0.582535  0.547456  0.130846 -0.116277   \n",
      "86  -0.827151 -0.241365  0.567809  0.057982  0.729536 -0.184280 -0.630507   \n",
      "0    0.934171  0.248095  1.270760 -0.706506 -1.182295 -0.622715 -0.077902   \n",
      "105  2.172393 -2.431694 -1.885491  0.905118 -0.649063  0.404868  3.368203   \n",
      "44  -0.108717 -0.865425  0.525632 -0.520549  0.339366  0.144547 -0.070227   \n",
      "178  0.033646  1.251486 -0.310880  0.367910 -0.401956 -0.704922  0.313526   \n",
      "48   1.338083 -0.278074  0.764635 -1.347023 -0.909176 -0.526808  0.896831   \n",
      "153 -0.691410 -0.021108  0.504543 -0.458564  0.001219  0.103443 -0.407930   \n",
      "70  -0.810597  1.740945  0.694340  0.615852 -1.065244 -0.485704 -1.144736   \n",
      "169  0.579920 -0.204655 -1.885491  0.657175  0.443411 -0.060970  1.894590   \n",
      "38   1.304975  0.945574  0.799783 -2.008201 -1.195300 -0.554210  0.551453   \n",
      "150 -0.509318 -0.363729  0.539691  0.657175 -0.271899  0.117144 -0.361879   \n",
      "156 -0.542426 -0.033344  0.497514 -0.334593 -0.063809  0.007536 -0.154653   \n",
      "163 -1.009242  0.700844 -0.001582  4.252335 -3.640362  1.596864 -2.288321   \n",
      "167  0.497151 -0.975553 -1.885491  0.429895  1.379818 -0.184280  2.055766   \n",
      "197 -0.304051  1.545161 -1.885491  1.855562  0.768552 -0.704922  0.075599   \n",
      "188  1.417541  1.740945 -0.338998  1.277031 -3.159153  0.336362  0.697280   \n",
      "64   1.169234  0.052311  0.743547 -1.119742 -0.883165 -0.458302  0.582154   \n",
      "68   1.103019 -0.388202  0.631074 -1.119742 -0.636057 -0.389797  0.743330   \n",
      "145  0.066753 -0.718587  0.694340 -0.417240 -0.154848  0.144547 -0.131627   \n",
      "158 -0.141824  0.113493  0.511573  0.161291 -0.844148  0.089742 -0.047202   \n",
      "194 -0.449725  1.373851 -1.885491  1.111736  0.781558 -0.704922 -0.254428   \n",
      "42  -0.131892 -0.278074  0.497514 -0.231283  0.092259  0.103443 -0.200703   \n",
      "160  0.043578 -0.131236  0.462366  0.202615 -0.714091  0.062340  0.106300   \n",
      "87  -0.575533 -0.045581  0.567809  0.161291 -0.050803  0.213052 -0.592131   \n",
      "113  0.242223  0.027838  0.806812 -0.375916 -0.180860  0.076041 -0.492355   \n",
      "63   1.351326  0.896628  0.792753 -1.367684 -1.741537 -0.704922  0.643554   \n",
      "79  -0.757625 -0.755297  0.588897  0.946441  0.222315  0.240454 -0.676557   \n",
      "133 -0.062366  0.333750  0.877107  0.202615 -1.143278  0.034938 -0.492355   \n",
      "175  0.993764 -0.571750 -1.653517  0.140630  0.911615 -0.526808  1.856215   \n",
      "166  1.099708 -2.945627 -0.683444  0.243939  0.976643  0.089742  2.124842   \n",
      "94  -0.628505 -0.889898  0.455337  0.099306  0.768552  0.213052 -0.469330   \n",
      "24  -0.327227 -0.070054  0.574838 -0.603196  0.209310 -0.019867 -0.323504   \n",
      "195 -0.906609  0.859919 -1.885491  2.558064  0.911615 -0.595313  0.167700   \n",
      "147 -0.691410 -0.131236  0.595927 -0.210622 -0.024792  0.062340 -0.400255   \n",
      "32  -0.145135 -0.718587  0.560779 -0.437902  0.365377  0.130846 -0.223728   \n",
      "\n",
      "            8         9  class  \n",
      "143 -0.374685 -0.574827      2  \n",
      "168 -0.374685 -0.574827      5  \n",
      "71  -0.374685  2.743229      2  \n",
      "54  -0.374685  0.358376      1  \n",
      "88  -0.374685 -0.574827      2  \n",
      "131 -0.374685  0.462065      2  \n",
      "124 -0.374685 -0.574827      2  \n",
      "21  -0.374685 -0.574827      1  \n",
      "14  -0.374685 -0.574827      1  \n",
      "151 -0.374685 -0.574827      3  \n",
      "196  1.165897 -0.471138      7  \n",
      "123 -0.374685 -0.574827      2  \n",
      "138 -0.374685 -0.574827      2  \n",
      "111 -0.374685 -0.574827      2  \n",
      "51  -0.374685  0.565755      1  \n",
      "112 -0.374685 -0.574827      2  \n",
      "9   -0.374685  0.565755      1  \n",
      "19  -0.374685  0.150997      1  \n",
      "16  -0.374685 -0.574827      1  \n",
      "86  -0.374685 -0.574827      2  \n",
      "0   -0.374685 -0.574827      1  \n",
      "105 -0.374685  2.950608      2  \n",
      "44  -0.374685  2.535851      1  \n",
      "178 -0.374685 -0.574827      6  \n",
      "48  -0.374685 -0.574827      1  \n",
      "153 -0.374685 -0.574827      3  \n",
      "70  -0.374685  0.669444      2  \n",
      "169 -0.374685 -0.574827      5  \n",
      "38  -0.374685 -0.574827      1  \n",
      "150 -0.374685  1.187890      3  \n",
      "156 -0.374685 -0.574827      3  \n",
      "163  3.809612 -0.574827      5  \n",
      "167 -0.374685 -0.574827      5  \n",
      "197  0.880604 -0.574827      7  \n",
      "188 -0.374685 -0.574827      7  \n",
      "64  -0.374685  0.150997      1  \n",
      "68  -0.374685  1.084201      1  \n",
      "145 -0.374685  3.054297      2  \n",
      "158 -0.374685 -0.574827      3  \n",
      "194  2.611381  0.150997      7  \n",
      "42  -0.374685 -0.574827      1  \n",
      "160 -0.374685 -0.574827      3  \n",
      "87  -0.374685  0.462065      2  \n",
      "113 -0.374685  0.876822      2  \n",
      "63  -0.374685 -0.574827      1  \n",
      "79  -0.374685 -0.574827      2  \n",
      "133 -0.374685  0.980512      2  \n",
      "175 -0.374685  2.328472      5  \n",
      "166 -0.374685 -0.574827      5  \n",
      "94  -0.374685 -0.574827      2  \n",
      "24  -0.374685 -0.574827      1  \n",
      "195  0.785507 -0.056381      7  \n",
      "147 -0.374685 -0.574827      3  \n",
      "32  -0.203509  1.706337      1  , 'train_size': 100, 'test_size': 54, 'train_df':             1         2         3         4         5         6         7  \\\n",
      "25  -0.181553 -0.559513  0.602956 -0.479225  0.404394  0.185650 -0.246753   \n",
      "208 -0.592087  1.141358 -1.885491  2.682035  0.209310 -0.704922  0.459353   \n",
      "122 -0.436482 -0.253601  0.602956  0.078644  0.196304  0.062340 -0.576781   \n",
      "50   1.659226  0.345986  0.729488 -1.925554 -1.221312 -0.581612  0.927532   \n",
      "174  0.791808 -0.718587 -0.753739  1.504311 -0.662069  0.336362  0.651229   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "35  -0.833772 -0.180182  0.539691 -0.479225  0.066248  0.062340 -0.216053   \n",
      "121 -0.515940 -0.620695  0.602956  0.367910  0.352372  0.171949 -0.630507   \n",
      "206 -0.575533  1.838837 -1.885491  0.884456  0.547456 -0.704922 -0.139302   \n",
      "132 -0.019327 -0.008871  0.912255 -0.541211 -0.258894  0.089742 -0.538406   \n",
      "183  0.497151  1.373851 -1.885491 -1.822245  1.028665 -0.704922  1.817839   \n",
      "\n",
      "            8         9  class  \n",
      "25  -0.374685 -0.574827      1  \n",
      "208  0.652370 -0.574827      7  \n",
      "122 -0.374685 -0.574827      2  \n",
      "50  -0.374685  1.084201      1  \n",
      "174  0.081784  4.713325      5  \n",
      "..        ...       ...    ...  \n",
      "35  -0.374685 -0.574827      1  \n",
      "121 -0.374685  1.602647      2  \n",
      "206  2.250010 -0.574827      7  \n",
      "132 -0.374685 -0.574827      2  \n",
      "183 -0.374685 -0.574827      6  \n",
      "\n",
      "[100 rows x 10 columns], 'test_df':             1         2         3         4         5         6         7  \\\n",
      "143 -0.363645 -0.535040  0.553750  0.719161  0.040236  0.199351 -0.515381   \n",
      "168 -0.506007 -0.706351 -1.885491  0.801808  1.548891  0.624085  1.011957   \n",
      "71   0.096550  0.248095  0.834930 -0.355254 -0.948193  0.034938 -0.407930   \n",
      "54  -0.135203 -0.278074  0.089802 -0.313931  0.378383 -0.006165  0.129325   \n",
      "88  -0.664924 -0.522804  0.574838  0.078644  0.261332  0.117144 -0.561431   \n",
      "131  2.632587  0.321514 -1.885491 -0.169298 -1.884600 -0.444601  3.521704   \n",
      "124  1.185788 -0.290310  0.701370 -0.603196  0.079253  0.034938 -0.254428   \n",
      "21   0.487219  1.630817  0.750576 -2.380114 -0.870159 -0.663819  0.113975   \n",
      "14  -0.184864 -1.012263  0.638104 -0.272607  0.781558  0.089742 -0.269779   \n",
      "151  1.020250  1.080175  0.856019 -1.264375 -1.546453 -0.704922  0.490053   \n",
      "196 -0.870190  0.529534 -1.885491  2.268799  0.703524 -0.513107  0.428652   \n",
      "123 -0.370266  0.052311  0.560779  0.553866 -0.219877  0.144547 -0.661207   \n",
      "138 -0.479521 -0.792006  0.588897  0.202615  0.872598  0.199351 -0.730283   \n",
      "111  3.046432 -2.957863 -1.885491 -1.429670  0.508440 -0.704922  4.688314   \n",
      "51   0.354789 -0.290310  0.455337 -0.334593 -0.427967  0.117144  0.221426   \n",
      "112  3.172240 -0.975553 -1.885491 -1.594965 -0.870159 -0.622715  4.258510   \n",
      "9   -0.211350 -0.535040  0.645133 -0.169298  0.391389  0.076041 -0.346529   \n",
      "19  -0.277565 -0.510567  0.602956  0.512543  0.053242  0.034938 -0.315829   \n",
      "16  -0.115338 -0.926607  0.694340 -0.582535  0.547456  0.130846 -0.116277   \n",
      "86  -0.827151 -0.241365  0.567809  0.057982  0.729536 -0.184280 -0.630507   \n",
      "0    0.934171  0.248095  1.270760 -0.706506 -1.182295 -0.622715 -0.077902   \n",
      "105  2.172393 -2.431694 -1.885491  0.905118 -0.649063  0.404868  3.368203   \n",
      "44  -0.108717 -0.865425  0.525632 -0.520549  0.339366  0.144547 -0.070227   \n",
      "178  0.033646  1.251486 -0.310880  0.367910 -0.401956 -0.704922  0.313526   \n",
      "48   1.338083 -0.278074  0.764635 -1.347023 -0.909176 -0.526808  0.896831   \n",
      "153 -0.691410 -0.021108  0.504543 -0.458564  0.001219  0.103443 -0.407930   \n",
      "70  -0.810597  1.740945  0.694340  0.615852 -1.065244 -0.485704 -1.144736   \n",
      "169  0.579920 -0.204655 -1.885491  0.657175  0.443411 -0.060970  1.894590   \n",
      "38   1.304975  0.945574  0.799783 -2.008201 -1.195300 -0.554210  0.551453   \n",
      "150 -0.509318 -0.363729  0.539691  0.657175 -0.271899  0.117144 -0.361879   \n",
      "156 -0.542426 -0.033344  0.497514 -0.334593 -0.063809  0.007536 -0.154653   \n",
      "163 -1.009242  0.700844 -0.001582  4.252335 -3.640362  1.596864 -2.288321   \n",
      "167  0.497151 -0.975553 -1.885491  0.429895  1.379818 -0.184280  2.055766   \n",
      "197 -0.304051  1.545161 -1.885491  1.855562  0.768552 -0.704922  0.075599   \n",
      "188  1.417541  1.740945 -0.338998  1.277031 -3.159153  0.336362  0.697280   \n",
      "64   1.169234  0.052311  0.743547 -1.119742 -0.883165 -0.458302  0.582154   \n",
      "68   1.103019 -0.388202  0.631074 -1.119742 -0.636057 -0.389797  0.743330   \n",
      "145  0.066753 -0.718587  0.694340 -0.417240 -0.154848  0.144547 -0.131627   \n",
      "158 -0.141824  0.113493  0.511573  0.161291 -0.844148  0.089742 -0.047202   \n",
      "194 -0.449725  1.373851 -1.885491  1.111736  0.781558 -0.704922 -0.254428   \n",
      "42  -0.131892 -0.278074  0.497514 -0.231283  0.092259  0.103443 -0.200703   \n",
      "160  0.043578 -0.131236  0.462366  0.202615 -0.714091  0.062340  0.106300   \n",
      "87  -0.575533 -0.045581  0.567809  0.161291 -0.050803  0.213052 -0.592131   \n",
      "113  0.242223  0.027838  0.806812 -0.375916 -0.180860  0.076041 -0.492355   \n",
      "63   1.351326  0.896628  0.792753 -1.367684 -1.741537 -0.704922  0.643554   \n",
      "79  -0.757625 -0.755297  0.588897  0.946441  0.222315  0.240454 -0.676557   \n",
      "133 -0.062366  0.333750  0.877107  0.202615 -1.143278  0.034938 -0.492355   \n",
      "175  0.993764 -0.571750 -1.653517  0.140630  0.911615 -0.526808  1.856215   \n",
      "166  1.099708 -2.945627 -0.683444  0.243939  0.976643  0.089742  2.124842   \n",
      "94  -0.628505 -0.889898  0.455337  0.099306  0.768552  0.213052 -0.469330   \n",
      "24  -0.327227 -0.070054  0.574838 -0.603196  0.209310 -0.019867 -0.323504   \n",
      "195 -0.906609  0.859919 -1.885491  2.558064  0.911615 -0.595313  0.167700   \n",
      "147 -0.691410 -0.131236  0.595927 -0.210622 -0.024792  0.062340 -0.400255   \n",
      "32  -0.145135 -0.718587  0.560779 -0.437902  0.365377  0.130846 -0.223728   \n",
      "\n",
      "            8         9  class  \n",
      "143 -0.374685 -0.574827      2  \n",
      "168 -0.374685 -0.574827      5  \n",
      "71  -0.374685  2.743229      2  \n",
      "54  -0.374685  0.358376      1  \n",
      "88  -0.374685 -0.574827      2  \n",
      "131 -0.374685  0.462065      2  \n",
      "124 -0.374685 -0.574827      2  \n",
      "21  -0.374685 -0.574827      1  \n",
      "14  -0.374685 -0.574827      1  \n",
      "151 -0.374685 -0.574827      3  \n",
      "196  1.165897 -0.471138      7  \n",
      "123 -0.374685 -0.574827      2  \n",
      "138 -0.374685 -0.574827      2  \n",
      "111 -0.374685 -0.574827      2  \n",
      "51  -0.374685  0.565755      1  \n",
      "112 -0.374685 -0.574827      2  \n",
      "9   -0.374685  0.565755      1  \n",
      "19  -0.374685  0.150997      1  \n",
      "16  -0.374685 -0.574827      1  \n",
      "86  -0.374685 -0.574827      2  \n",
      "0   -0.374685 -0.574827      1  \n",
      "105 -0.374685  2.950608      2  \n",
      "44  -0.374685  2.535851      1  \n",
      "178 -0.374685 -0.574827      6  \n",
      "48  -0.374685 -0.574827      1  \n",
      "153 -0.374685 -0.574827      3  \n",
      "70  -0.374685  0.669444      2  \n",
      "169 -0.374685 -0.574827      5  \n",
      "38  -0.374685 -0.574827      1  \n",
      "150 -0.374685  1.187890      3  \n",
      "156 -0.374685 -0.574827      3  \n",
      "163  3.809612 -0.574827      5  \n",
      "167 -0.374685 -0.574827      5  \n",
      "197  0.880604 -0.574827      7  \n",
      "188 -0.374685 -0.574827      7  \n",
      "64  -0.374685  0.150997      1  \n",
      "68  -0.374685  1.084201      1  \n",
      "145 -0.374685  3.054297      2  \n",
      "158 -0.374685 -0.574827      3  \n",
      "194  2.611381  0.150997      7  \n",
      "42  -0.374685 -0.574827      1  \n",
      "160 -0.374685 -0.574827      3  \n",
      "87  -0.374685  0.462065      2  \n",
      "113 -0.374685  0.876822      2  \n",
      "63  -0.374685 -0.574827      1  \n",
      "79  -0.374685 -0.574827      2  \n",
      "133 -0.374685  0.980512      2  \n",
      "175 -0.374685  2.328472      5  \n",
      "166 -0.374685 -0.574827      5  \n",
      "94  -0.374685 -0.574827      2  \n",
      "24  -0.374685 -0.574827      1  \n",
      "195  0.785507 -0.056381      7  \n",
      "147 -0.374685 -0.574827      3  \n",
      "32  -0.203509  1.706337      1  , 'nclasses': 6, 'dt_len': 4.4, 'dt_acc': 0.6851851851851852, 'dt_clf': DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'lr_len': 5, 'lr_acc': 0.6481481481481481, 'lr_clf': LogisticRegression(C=0.20000000000000018, n_jobs=-1, penalty='l1',\n",
      "                   random_state=RandomState(MT19937) at 0x7F2CC67B7160,\n",
      "                   solver='liblinear')})])\n"
     ]
    }
   ],
   "source": [
    "print(datasets_info_dict.items())\n",
    "for dataset, dataset_info in datasets_info_dict.items():\n",
    "    print('---', dataset)\n",
    "    print('---', dataset_info)\n",
    "    train_df, test_df = get_data(dataset, dataset_info)\n",
    "    dataset_info['train_df'] = train_df\n",
    "    dataset_info['test_df'] = test_df\n",
    "    dataset_info['nclasses'] = train_df['class'].unique().shape[0]\n",
    "    \n",
    "print(datasets_info_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.226153Z",
     "start_time": "2020-10-28T08:15:39.057630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FOLDER_NAME': 'glass',\n",
       " 'SEPARATE_FILES': False,\n",
       " 'TRAIN_FILENAME': None,\n",
       " 'TEST_FILENAME': None,\n",
       " 'COMBINED_FILENAME': 'glass.data',\n",
       " 'CLASS_INDEX': -1,\n",
       " 'HAS_INDEX': True,\n",
       " 'HEADER_ROW_NUMBER': None,\n",
       " 'initial_features': 9,\n",
       " '_NUM': 9,\n",
       " '_CAT': 0,\n",
       " 'features': 9,\n",
       " 'initial_train_size': 160,\n",
       " 'initial_test_size': 54,\n",
       " 'original_train_df':             1         2         3         4         5         6         7  \\\n",
       " 25  -0.181553 -0.559513  0.602956 -0.479225  0.404394  0.185650 -0.246753   \n",
       " 208 -0.592087  1.141358 -1.885491  2.682035  0.209310 -0.704922  0.459353   \n",
       " 122 -0.436482 -0.253601  0.602956  0.078644  0.196304  0.062340 -0.576781   \n",
       " 50   1.659226  0.345986  0.729488 -1.925554 -1.221312 -0.581612  0.927532   \n",
       " 174  0.791808 -0.718587 -0.753739  1.504311 -0.662069  0.336362  0.651229   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 37  -0.072299 -0.853189  0.560779 -0.189960  0.352372  0.171949 -0.131627   \n",
       " 182  0.321681  0.872155 -1.885491  1.339016  0.066248 -0.704922  1.556887   \n",
       " 114  0.093239 -0.412675  0.905225 -0.520549 -0.323922  0.117144 -0.323504   \n",
       " 53   0.060132 -0.363729  0.110891 -0.334593  0.209310  0.048639  0.167700   \n",
       " 185 -2.277261  0.309277  0.363953  0.760485  0.157287  1.706473 -2.626024   \n",
       " \n",
       "             8         9  class  \n",
       " 25  -0.374685 -0.574827      1  \n",
       " 208  0.652370 -0.574827      7  \n",
       " 122 -0.374685 -0.574827      2  \n",
       " 50  -0.374685  1.084201      1  \n",
       " 174  0.081784  4.713325      5  \n",
       " ..        ...       ...    ...  \n",
       " 37  -0.374685 -0.574827      1  \n",
       " 182 -0.374685 -0.574827      6  \n",
       " 114 -0.374685 -0.574827      2  \n",
       " 53  -0.374685 -0.574827      1  \n",
       " 185  1.888639 -0.574827      7  \n",
       " \n",
       " [160 rows x 10 columns],\n",
       " 'original_test_df':             1         2         3         4         5         6         7  \\\n",
       " 143 -0.363645 -0.535040  0.553750  0.719161  0.040236  0.199351 -0.515381   \n",
       " 168 -0.506007 -0.706351 -1.885491  0.801808  1.548891  0.624085  1.011957   \n",
       " 71   0.096550  0.248095  0.834930 -0.355254 -0.948193  0.034938 -0.407930   \n",
       " 54  -0.135203 -0.278074  0.089802 -0.313931  0.378383 -0.006165  0.129325   \n",
       " 88  -0.664924 -0.522804  0.574838  0.078644  0.261332  0.117144 -0.561431   \n",
       " 131  2.632587  0.321514 -1.885491 -0.169298 -1.884600 -0.444601  3.521704   \n",
       " 124  1.185788 -0.290310  0.701370 -0.603196  0.079253  0.034938 -0.254428   \n",
       " 21   0.487219  1.630817  0.750576 -2.380114 -0.870159 -0.663819  0.113975   \n",
       " 14  -0.184864 -1.012263  0.638104 -0.272607  0.781558  0.089742 -0.269779   \n",
       " 151  1.020250  1.080175  0.856019 -1.264375 -1.546453 -0.704922  0.490053   \n",
       " 196 -0.870190  0.529534 -1.885491  2.268799  0.703524 -0.513107  0.428652   \n",
       " 123 -0.370266  0.052311  0.560779  0.553866 -0.219877  0.144547 -0.661207   \n",
       " 138 -0.479521 -0.792006  0.588897  0.202615  0.872598  0.199351 -0.730283   \n",
       " 111  3.046432 -2.957863 -1.885491 -1.429670  0.508440 -0.704922  4.688314   \n",
       " 51   0.354789 -0.290310  0.455337 -0.334593 -0.427967  0.117144  0.221426   \n",
       " 112  3.172240 -0.975553 -1.885491 -1.594965 -0.870159 -0.622715  4.258510   \n",
       " 9   -0.211350 -0.535040  0.645133 -0.169298  0.391389  0.076041 -0.346529   \n",
       " 19  -0.277565 -0.510567  0.602956  0.512543  0.053242  0.034938 -0.315829   \n",
       " 16  -0.115338 -0.926607  0.694340 -0.582535  0.547456  0.130846 -0.116277   \n",
       " 86  -0.827151 -0.241365  0.567809  0.057982  0.729536 -0.184280 -0.630507   \n",
       " 0    0.934171  0.248095  1.270760 -0.706506 -1.182295 -0.622715 -0.077902   \n",
       " 105  2.172393 -2.431694 -1.885491  0.905118 -0.649063  0.404868  3.368203   \n",
       " 44  -0.108717 -0.865425  0.525632 -0.520549  0.339366  0.144547 -0.070227   \n",
       " 178  0.033646  1.251486 -0.310880  0.367910 -0.401956 -0.704922  0.313526   \n",
       " 48   1.338083 -0.278074  0.764635 -1.347023 -0.909176 -0.526808  0.896831   \n",
       " 153 -0.691410 -0.021108  0.504543 -0.458564  0.001219  0.103443 -0.407930   \n",
       " 70  -0.810597  1.740945  0.694340  0.615852 -1.065244 -0.485704 -1.144736   \n",
       " 169  0.579920 -0.204655 -1.885491  0.657175  0.443411 -0.060970  1.894590   \n",
       " 38   1.304975  0.945574  0.799783 -2.008201 -1.195300 -0.554210  0.551453   \n",
       " 150 -0.509318 -0.363729  0.539691  0.657175 -0.271899  0.117144 -0.361879   \n",
       " 156 -0.542426 -0.033344  0.497514 -0.334593 -0.063809  0.007536 -0.154653   \n",
       " 163 -1.009242  0.700844 -0.001582  4.252335 -3.640362  1.596864 -2.288321   \n",
       " 167  0.497151 -0.975553 -1.885491  0.429895  1.379818 -0.184280  2.055766   \n",
       " 197 -0.304051  1.545161 -1.885491  1.855562  0.768552 -0.704922  0.075599   \n",
       " 188  1.417541  1.740945 -0.338998  1.277031 -3.159153  0.336362  0.697280   \n",
       " 64   1.169234  0.052311  0.743547 -1.119742 -0.883165 -0.458302  0.582154   \n",
       " 68   1.103019 -0.388202  0.631074 -1.119742 -0.636057 -0.389797  0.743330   \n",
       " 145  0.066753 -0.718587  0.694340 -0.417240 -0.154848  0.144547 -0.131627   \n",
       " 158 -0.141824  0.113493  0.511573  0.161291 -0.844148  0.089742 -0.047202   \n",
       " 194 -0.449725  1.373851 -1.885491  1.111736  0.781558 -0.704922 -0.254428   \n",
       " 42  -0.131892 -0.278074  0.497514 -0.231283  0.092259  0.103443 -0.200703   \n",
       " 160  0.043578 -0.131236  0.462366  0.202615 -0.714091  0.062340  0.106300   \n",
       " 87  -0.575533 -0.045581  0.567809  0.161291 -0.050803  0.213052 -0.592131   \n",
       " 113  0.242223  0.027838  0.806812 -0.375916 -0.180860  0.076041 -0.492355   \n",
       " 63   1.351326  0.896628  0.792753 -1.367684 -1.741537 -0.704922  0.643554   \n",
       " 79  -0.757625 -0.755297  0.588897  0.946441  0.222315  0.240454 -0.676557   \n",
       " 133 -0.062366  0.333750  0.877107  0.202615 -1.143278  0.034938 -0.492355   \n",
       " 175  0.993764 -0.571750 -1.653517  0.140630  0.911615 -0.526808  1.856215   \n",
       " 166  1.099708 -2.945627 -0.683444  0.243939  0.976643  0.089742  2.124842   \n",
       " 94  -0.628505 -0.889898  0.455337  0.099306  0.768552  0.213052 -0.469330   \n",
       " 24  -0.327227 -0.070054  0.574838 -0.603196  0.209310 -0.019867 -0.323504   \n",
       " 195 -0.906609  0.859919 -1.885491  2.558064  0.911615 -0.595313  0.167700   \n",
       " 147 -0.691410 -0.131236  0.595927 -0.210622 -0.024792  0.062340 -0.400255   \n",
       " 32  -0.145135 -0.718587  0.560779 -0.437902  0.365377  0.130846 -0.223728   \n",
       " \n",
       "             8         9  class  \n",
       " 143 -0.374685 -0.574827      2  \n",
       " 168 -0.374685 -0.574827      5  \n",
       " 71  -0.374685  2.743229      2  \n",
       " 54  -0.374685  0.358376      1  \n",
       " 88  -0.374685 -0.574827      2  \n",
       " 131 -0.374685  0.462065      2  \n",
       " 124 -0.374685 -0.574827      2  \n",
       " 21  -0.374685 -0.574827      1  \n",
       " 14  -0.374685 -0.574827      1  \n",
       " 151 -0.374685 -0.574827      3  \n",
       " 196  1.165897 -0.471138      7  \n",
       " 123 -0.374685 -0.574827      2  \n",
       " 138 -0.374685 -0.574827      2  \n",
       " 111 -0.374685 -0.574827      2  \n",
       " 51  -0.374685  0.565755      1  \n",
       " 112 -0.374685 -0.574827      2  \n",
       " 9   -0.374685  0.565755      1  \n",
       " 19  -0.374685  0.150997      1  \n",
       " 16  -0.374685 -0.574827      1  \n",
       " 86  -0.374685 -0.574827      2  \n",
       " 0   -0.374685 -0.574827      1  \n",
       " 105 -0.374685  2.950608      2  \n",
       " 44  -0.374685  2.535851      1  \n",
       " 178 -0.374685 -0.574827      6  \n",
       " 48  -0.374685 -0.574827      1  \n",
       " 153 -0.374685 -0.574827      3  \n",
       " 70  -0.374685  0.669444      2  \n",
       " 169 -0.374685 -0.574827      5  \n",
       " 38  -0.374685 -0.574827      1  \n",
       " 150 -0.374685  1.187890      3  \n",
       " 156 -0.374685 -0.574827      3  \n",
       " 163  3.809612 -0.574827      5  \n",
       " 167 -0.374685 -0.574827      5  \n",
       " 197  0.880604 -0.574827      7  \n",
       " 188 -0.374685 -0.574827      7  \n",
       " 64  -0.374685  0.150997      1  \n",
       " 68  -0.374685  1.084201      1  \n",
       " 145 -0.374685  3.054297      2  \n",
       " 158 -0.374685 -0.574827      3  \n",
       " 194  2.611381  0.150997      7  \n",
       " 42  -0.374685 -0.574827      1  \n",
       " 160 -0.374685 -0.574827      3  \n",
       " 87  -0.374685  0.462065      2  \n",
       " 113 -0.374685  0.876822      2  \n",
       " 63  -0.374685 -0.574827      1  \n",
       " 79  -0.374685 -0.574827      2  \n",
       " 133 -0.374685  0.980512      2  \n",
       " 175 -0.374685  2.328472      5  \n",
       " 166 -0.374685 -0.574827      5  \n",
       " 94  -0.374685 -0.574827      2  \n",
       " 24  -0.374685 -0.574827      1  \n",
       " 195  0.785507 -0.056381      7  \n",
       " 147 -0.374685 -0.574827      3  \n",
       " 32  -0.203509  1.706337      1  ,\n",
       " 'train_size': 100,\n",
       " 'test_size': 54,\n",
       " 'train_df':             1         2         3         4         5         6         7  \\\n",
       " 25  -0.181553 -0.559513  0.602956 -0.479225  0.404394  0.185650 -0.246753   \n",
       " 208 -0.592087  1.141358 -1.885491  2.682035  0.209310 -0.704922  0.459353   \n",
       " 122 -0.436482 -0.253601  0.602956  0.078644  0.196304  0.062340 -0.576781   \n",
       " 50   1.659226  0.345986  0.729488 -1.925554 -1.221312 -0.581612  0.927532   \n",
       " 174  0.791808 -0.718587 -0.753739  1.504311 -0.662069  0.336362  0.651229   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 35  -0.833772 -0.180182  0.539691 -0.479225  0.066248  0.062340 -0.216053   \n",
       " 121 -0.515940 -0.620695  0.602956  0.367910  0.352372  0.171949 -0.630507   \n",
       " 206 -0.575533  1.838837 -1.885491  0.884456  0.547456 -0.704922 -0.139302   \n",
       " 132 -0.019327 -0.008871  0.912255 -0.541211 -0.258894  0.089742 -0.538406   \n",
       " 183  0.497151  1.373851 -1.885491 -1.822245  1.028665 -0.704922  1.817839   \n",
       " \n",
       "             8         9  class  \n",
       " 25  -0.374685 -0.574827      1  \n",
       " 208  0.652370 -0.574827      7  \n",
       " 122 -0.374685 -0.574827      2  \n",
       " 50  -0.374685  1.084201      1  \n",
       " 174  0.081784  4.713325      5  \n",
       " ..        ...       ...    ...  \n",
       " 35  -0.374685 -0.574827      1  \n",
       " 121 -0.374685  1.602647      2  \n",
       " 206  2.250010 -0.574827      7  \n",
       " 132 -0.374685 -0.574827      2  \n",
       " 183 -0.374685 -0.574827      6  \n",
       " \n",
       " [100 rows x 10 columns],\n",
       " 'test_df':             1         2         3         4         5         6         7  \\\n",
       " 143 -0.363645 -0.535040  0.553750  0.719161  0.040236  0.199351 -0.515381   \n",
       " 168 -0.506007 -0.706351 -1.885491  0.801808  1.548891  0.624085  1.011957   \n",
       " 71   0.096550  0.248095  0.834930 -0.355254 -0.948193  0.034938 -0.407930   \n",
       " 54  -0.135203 -0.278074  0.089802 -0.313931  0.378383 -0.006165  0.129325   \n",
       " 88  -0.664924 -0.522804  0.574838  0.078644  0.261332  0.117144 -0.561431   \n",
       " 131  2.632587  0.321514 -1.885491 -0.169298 -1.884600 -0.444601  3.521704   \n",
       " 124  1.185788 -0.290310  0.701370 -0.603196  0.079253  0.034938 -0.254428   \n",
       " 21   0.487219  1.630817  0.750576 -2.380114 -0.870159 -0.663819  0.113975   \n",
       " 14  -0.184864 -1.012263  0.638104 -0.272607  0.781558  0.089742 -0.269779   \n",
       " 151  1.020250  1.080175  0.856019 -1.264375 -1.546453 -0.704922  0.490053   \n",
       " 196 -0.870190  0.529534 -1.885491  2.268799  0.703524 -0.513107  0.428652   \n",
       " 123 -0.370266  0.052311  0.560779  0.553866 -0.219877  0.144547 -0.661207   \n",
       " 138 -0.479521 -0.792006  0.588897  0.202615  0.872598  0.199351 -0.730283   \n",
       " 111  3.046432 -2.957863 -1.885491 -1.429670  0.508440 -0.704922  4.688314   \n",
       " 51   0.354789 -0.290310  0.455337 -0.334593 -0.427967  0.117144  0.221426   \n",
       " 112  3.172240 -0.975553 -1.885491 -1.594965 -0.870159 -0.622715  4.258510   \n",
       " 9   -0.211350 -0.535040  0.645133 -0.169298  0.391389  0.076041 -0.346529   \n",
       " 19  -0.277565 -0.510567  0.602956  0.512543  0.053242  0.034938 -0.315829   \n",
       " 16  -0.115338 -0.926607  0.694340 -0.582535  0.547456  0.130846 -0.116277   \n",
       " 86  -0.827151 -0.241365  0.567809  0.057982  0.729536 -0.184280 -0.630507   \n",
       " 0    0.934171  0.248095  1.270760 -0.706506 -1.182295 -0.622715 -0.077902   \n",
       " 105  2.172393 -2.431694 -1.885491  0.905118 -0.649063  0.404868  3.368203   \n",
       " 44  -0.108717 -0.865425  0.525632 -0.520549  0.339366  0.144547 -0.070227   \n",
       " 178  0.033646  1.251486 -0.310880  0.367910 -0.401956 -0.704922  0.313526   \n",
       " 48   1.338083 -0.278074  0.764635 -1.347023 -0.909176 -0.526808  0.896831   \n",
       " 153 -0.691410 -0.021108  0.504543 -0.458564  0.001219  0.103443 -0.407930   \n",
       " 70  -0.810597  1.740945  0.694340  0.615852 -1.065244 -0.485704 -1.144736   \n",
       " 169  0.579920 -0.204655 -1.885491  0.657175  0.443411 -0.060970  1.894590   \n",
       " 38   1.304975  0.945574  0.799783 -2.008201 -1.195300 -0.554210  0.551453   \n",
       " 150 -0.509318 -0.363729  0.539691  0.657175 -0.271899  0.117144 -0.361879   \n",
       " 156 -0.542426 -0.033344  0.497514 -0.334593 -0.063809  0.007536 -0.154653   \n",
       " 163 -1.009242  0.700844 -0.001582  4.252335 -3.640362  1.596864 -2.288321   \n",
       " 167  0.497151 -0.975553 -1.885491  0.429895  1.379818 -0.184280  2.055766   \n",
       " 197 -0.304051  1.545161 -1.885491  1.855562  0.768552 -0.704922  0.075599   \n",
       " 188  1.417541  1.740945 -0.338998  1.277031 -3.159153  0.336362  0.697280   \n",
       " 64   1.169234  0.052311  0.743547 -1.119742 -0.883165 -0.458302  0.582154   \n",
       " 68   1.103019 -0.388202  0.631074 -1.119742 -0.636057 -0.389797  0.743330   \n",
       " 145  0.066753 -0.718587  0.694340 -0.417240 -0.154848  0.144547 -0.131627   \n",
       " 158 -0.141824  0.113493  0.511573  0.161291 -0.844148  0.089742 -0.047202   \n",
       " 194 -0.449725  1.373851 -1.885491  1.111736  0.781558 -0.704922 -0.254428   \n",
       " 42  -0.131892 -0.278074  0.497514 -0.231283  0.092259  0.103443 -0.200703   \n",
       " 160  0.043578 -0.131236  0.462366  0.202615 -0.714091  0.062340  0.106300   \n",
       " 87  -0.575533 -0.045581  0.567809  0.161291 -0.050803  0.213052 -0.592131   \n",
       " 113  0.242223  0.027838  0.806812 -0.375916 -0.180860  0.076041 -0.492355   \n",
       " 63   1.351326  0.896628  0.792753 -1.367684 -1.741537 -0.704922  0.643554   \n",
       " 79  -0.757625 -0.755297  0.588897  0.946441  0.222315  0.240454 -0.676557   \n",
       " 133 -0.062366  0.333750  0.877107  0.202615 -1.143278  0.034938 -0.492355   \n",
       " 175  0.993764 -0.571750 -1.653517  0.140630  0.911615 -0.526808  1.856215   \n",
       " 166  1.099708 -2.945627 -0.683444  0.243939  0.976643  0.089742  2.124842   \n",
       " 94  -0.628505 -0.889898  0.455337  0.099306  0.768552  0.213052 -0.469330   \n",
       " 24  -0.327227 -0.070054  0.574838 -0.603196  0.209310 -0.019867 -0.323504   \n",
       " 195 -0.906609  0.859919 -1.885491  2.558064  0.911615 -0.595313  0.167700   \n",
       " 147 -0.691410 -0.131236  0.595927 -0.210622 -0.024792  0.062340 -0.400255   \n",
       " 32  -0.145135 -0.718587  0.560779 -0.437902  0.365377  0.130846 -0.223728   \n",
       " \n",
       "             8         9  class  \n",
       " 143 -0.374685 -0.574827      2  \n",
       " 168 -0.374685 -0.574827      5  \n",
       " 71  -0.374685  2.743229      2  \n",
       " 54  -0.374685  0.358376      1  \n",
       " 88  -0.374685 -0.574827      2  \n",
       " 131 -0.374685  0.462065      2  \n",
       " 124 -0.374685 -0.574827      2  \n",
       " 21  -0.374685 -0.574827      1  \n",
       " 14  -0.374685 -0.574827      1  \n",
       " 151 -0.374685 -0.574827      3  \n",
       " 196  1.165897 -0.471138      7  \n",
       " 123 -0.374685 -0.574827      2  \n",
       " 138 -0.374685 -0.574827      2  \n",
       " 111 -0.374685 -0.574827      2  \n",
       " 51  -0.374685  0.565755      1  \n",
       " 112 -0.374685 -0.574827      2  \n",
       " 9   -0.374685  0.565755      1  \n",
       " 19  -0.374685  0.150997      1  \n",
       " 16  -0.374685 -0.574827      1  \n",
       " 86  -0.374685 -0.574827      2  \n",
       " 0   -0.374685 -0.574827      1  \n",
       " 105 -0.374685  2.950608      2  \n",
       " 44  -0.374685  2.535851      1  \n",
       " 178 -0.374685 -0.574827      6  \n",
       " 48  -0.374685 -0.574827      1  \n",
       " 153 -0.374685 -0.574827      3  \n",
       " 70  -0.374685  0.669444      2  \n",
       " 169 -0.374685 -0.574827      5  \n",
       " 38  -0.374685 -0.574827      1  \n",
       " 150 -0.374685  1.187890      3  \n",
       " 156 -0.374685 -0.574827      3  \n",
       " 163  3.809612 -0.574827      5  \n",
       " 167 -0.374685 -0.574827      5  \n",
       " 197  0.880604 -0.574827      7  \n",
       " 188 -0.374685 -0.574827      7  \n",
       " 64  -0.374685  0.150997      1  \n",
       " 68  -0.374685  1.084201      1  \n",
       " 145 -0.374685  3.054297      2  \n",
       " 158 -0.374685 -0.574827      3  \n",
       " 194  2.611381  0.150997      7  \n",
       " 42  -0.374685 -0.574827      1  \n",
       " 160 -0.374685 -0.574827      3  \n",
       " 87  -0.374685  0.462065      2  \n",
       " 113 -0.374685  0.876822      2  \n",
       " 63  -0.374685 -0.574827      1  \n",
       " 79  -0.374685 -0.574827      2  \n",
       " 133 -0.374685  0.980512      2  \n",
       " 175 -0.374685  2.328472      5  \n",
       " 166 -0.374685 -0.574827      5  \n",
       " 94  -0.374685 -0.574827      2  \n",
       " 24  -0.374685 -0.574827      1  \n",
       " 195  0.785507 -0.056381      7  \n",
       " 147 -0.374685 -0.574827      3  \n",
       " 32  -0.203509  1.706337      1  ,\n",
       " 'nclasses': 6,\n",
       " 'dt_len': 4.4,\n",
       " 'dt_acc': 0.6851851851851852,\n",
       " 'dt_clf': DecisionTreeClassifier(max_depth=5,\n",
       "                        random_state=RandomState(MT19937) at 0x7F2CC67B7160),\n",
       " 'lr_len': 5,\n",
       " 'lr_acc': 0.6481481481481481,\n",
       " 'lr_clf': LogisticRegression(C=0.20000000000000018, n_jobs=-1, penalty='l1',\n",
       "                    random_state=RandomState(MT19937) at 0x7F2CC67B7160,\n",
       "                    solver='liblinear')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset, dataset_info in datasets_info_dict.items():\n",
    "    ############ Decision Tree ############\n",
    "    print('111')\n",
    "    if 'dt_clf' not in dataset_info:\n",
    "        print('dt_clf')\n",
    "        dt_clf = get_clf(dataset_info['original_train_df'], 'dt', info)\n",
    "        avg_explanation_len = get_dt_avg_explanation(dt_clf, dataset_info['train_df'].drop(columns=['class']))\n",
    "        print('avg_explanation_len = ', avg_explanation_len)\n",
    "        dataset_info['dt_len'] = avg_explanation_len\n",
    "        dataset_info['dt_acc'] = test_classifier_dt(dt_clf, dataset_info['test_df'])\n",
    "        dataset_info['dt_clf'] = dt_clf\n",
    "    ############ Logistic Regression ############\n",
    "    if 'lr_clf' not in dataset_info:\n",
    "        print('lr_clf')\n",
    "        lr_clf = get_clf(dataset_info['original_train_df'], 'lr', info)\n",
    "        avg_explanation_len = get_lr_avg_explanation(lr_clf, dataset_info['train_df'].drop(columns=['class']))\n",
    "        dataset_info['lr_len'] = avg_explanation_len\n",
    "        dataset_info['lr_acc'] = test_classifier_lr(lr_clf, dataset_info['test_df'])\n",
    "        dataset_info['lr_clf'] = lr_clf\n",
    "    print('222')\n",
    "    \n",
    "dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.297860Z",
     "start_time": "2020-10-28T08:15:39.227753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Datasets:', len(experiments_config.all_datasets_info_dict), '-->', len(datasets_info_dict))\n",
    "# for dataset, dataset_info in datasets_info_dict.items():\n",
    "#     print(dataset)\n",
    "#     pprint({x:y for (x,y) in dataset_info.items() if ('_df' not in x) and ('_clf' not in x)})\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.595220Z",
     "start_time": "2020-10-28T08:15:39.299264Z"
    }
   },
   "outputs": [],
   "source": [
    "# raise Exception(\"stop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuning BARBE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each instance and extract explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.759561Z",
     "start_time": "2020-10-28T08:15:39.596550Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nonzero_features_lime(row, clf, num_features, explainer, num_labels, \n",
    "                              max_valid_features, predicted_label_index, num_samples, xlime_mode, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    classes = list(range(num_labels))\n",
    "    print('** classes ', classes)\n",
    "#     if isinstance(explainer, xlime.lime_tabular.LimeTabularExplainer):\n",
    "#         print('Caller 1')\n",
    "#         x = explainer.explain_instance(row.to_numpy(), \n",
    "#                                    clf.predict_proba, \n",
    "#                                    num_features=max_valid_features, \n",
    "#                                    labels=[predicted_label_index],\n",
    "#                                    num_samples=num_samples,\n",
    "#                                    classes=classes,\n",
    "#                                    xlime_mode=xlime_mode,\n",
    "#                                    model_regressor='alaki' # SigDirect\n",
    "# #                                    model_regressor=sklearn.tree.DecisionTreeRegressor(\n",
    "# #                                                        criterion=\"mae\",\n",
    "# #                                                        max_depth=max_valid_features, \n",
    "# #                                        )\n",
    "# #                                   model_regressor=sklearn.linear_model.LinearRegression()\n",
    "# #                                   model_regressor=GaussianNB()\n",
    "#                                   )\n",
    "# #         print(x)\n",
    "#         fidelity = x.fidelity\n",
    "#     else:\n",
    "    print('Caller 2')\n",
    "    x = explainer.explain_instance(row.to_numpy(), \n",
    "                               clf.predict_proba, \n",
    "                               num_features=max_valid_features, \n",
    "                               labels=[predicted_label_index],\n",
    "                               num_samples=num_samples)\n",
    "#         print(x.local_pred)\n",
    "#         fidelity = 1.0\n",
    "    if x.local_pred>=1.0/num_labels:\n",
    "        fidelity = 1.0\n",
    "    else:\n",
    "        fidelity = 0.0\n",
    "    feature_score_pairs = x.as_map()[predicted_label_index]\n",
    "#     print('feature_score_pairs.shape', len(feature_score_pairs))\n",
    "    feature_score_pairs = [(x[0]+1, x[1]) for x in feature_score_pairs if x[1]!=0.]\n",
    "    feature_score_pairs = sorted(feature_score_pairs, key=lambda x:x[1], reverse=True)\n",
    "    nonzero_features = [x[0] for x in feature_score_pairs[:max_valid_features]]\n",
    "\n",
    "    if fidelity==0.0:##\n",
    "        nonzero_features = []\n",
    "    return nonzero_features, fidelity\n",
    "\n",
    "def get_nonzero_features_anchor(row, clf, num_features, explainer, \n",
    "                              max_valid_features, predicted_label_index, num_samples, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "#         def explain_instance(self, data_row, classifier_fn, threshold=0.95,\n",
    "#                           delta=0.1, tau=0.15, batch_size=100,\n",
    "#                           max_anchor_size=None,\n",
    "#                           desired_label=None,\n",
    "#                           beam_size=4, **kwargs):\n",
    "\n",
    "#     print('shape:', row.to_numpy().shape)\n",
    "    x = explainer.explain_instance(row.to_numpy().reshape(1,-1), \n",
    "                                   clf.predict, \n",
    "                                   max_anchor_size=max_valid_features, \n",
    "                                   desired_label=predicted_label_index,\n",
    "#                                    num_samples=num_samples,\n",
    "                                   coverage_samples=num_samples,\n",
    "                                  )\n",
    "#     print(x.names(), x.features())\n",
    "    features = x.features()#[predicted_label_index]\n",
    "    features = [x+1 for x in features]\n",
    "#     feature_score_pairs = [(x[0]+1, x[1]) for x in feature_score_pairs if x[1]!=0.]\n",
    "#     nonzero_features = [x[0] for x in feature_score_pairs]\n",
    "    return features\n",
    "\n",
    "def get_nonzero_features_shap(row, clf, num_features, explainer, \n",
    "                              max_valid_features, predicted_label_index, num_samples, seed):\n",
    "    \"\"\" For a given class, return top k features\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    x = explainer.shap_values(row, nsamples=num_samples)\n",
    "    features_scores = x[predicted_label_index]\n",
    "#     print(features_scores)\n",
    "    sorted_features = sorted(list(enumerate(features_scores)), key=lambda x:abs(x[1]), reverse=True)\n",
    "    ret = [x[0]+1 for x in sorted_features[:max_valid_features]]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:39.865783Z",
     "start_time": "2020-10-28T08:15:39.761157Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_instance(args):    \n",
    "    ((idx, row), clf, classifier_type, num_features, num_labels, \n",
    "                      explainer, num_samples, xlime_mode, seed, ordered_class_labels, method, max_features) = args\n",
    "    print('Evaluating: ', ((idx, row), clf, classifier_type, num_features, num_labels, \n",
    "                      explainer, num_samples, xlime_mode, seed, ordered_class_labels, method, max_features))\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "#     print(idx)\n",
    "    fidelity = 1\n",
    "    predicted_label_index = ordered_class_labels.index(clf.predict(row.values.reshape(1, -1))[0])\n",
    "    print(predicted_label_index, clf.predict(row.values.reshape(1, -1))[0])\n",
    "    clf_features = get_features(classifier_type, clf, row, {'max_features':max_features, 'num_labels':num_labels, 'predicted_label_index':predicted_label_index})\n",
    "    if method==\"XLIME\":\n",
    "#         my_modes = [[\"FOURTEEN\", \"SIXTEEN\"], [\"SEVENTEEN\", \"EIGHTEEN\"], \"FIFTEEN\" ]\n",
    "#         my_modes = [[\"FOURTEEN\", \"SEVENTEEN\"]] \n",
    "        my_modes = [\"FOURTEEN\", \"SEVENTEEN\", \"SIXTEEN\", \"EIGHTEEN\"]\n",
    "        explainer_features = []\n",
    "        temp_seed = seed\n",
    "        for i in range(REPEAT_COUNT):\n",
    "            b, fidelity = get_nonzero_features_lime(row, clf, num_features, \n",
    "                                                  explainer, num_labels, max_features,                                                      \n",
    "                                                  predicted_label_index, num_samples, \n",
    "#                                                   [\"FOURTEEN\",\"FIFTEEN\"],\n",
    "                                                  my_modes[i] , # \"FOURTEEN\"\n",
    "                                                  seed, # temp_seed, # seed, # \n",
    "                                                   )\n",
    "            temp_seed += 11\n",
    "            if fidelity==1.0:\n",
    "#                 if i>0:\n",
    "#                     print(i)\n",
    "                if len(explainer_features)>0:\n",
    "                    explainer_features.extend([x for x in b if x not in set(explainer_features)])\n",
    "                else:\n",
    "                    explainer_features = b\n",
    "                break\n",
    "        \n",
    "        explainer_features = explainer_features[:max_features]\n",
    "    elif method==\"LIME\":\n",
    "        print('----')\n",
    "        explainer_features, fidelity = get_nonzero_features_lime(row, clf, num_features, \n",
    "                                                  explainer, num_labels, max_features,                                                      \n",
    "                                                  predicted_label_index, num_samples, xlime_mode,\n",
    "                                                  seed)\n",
    "\n",
    "    elif method==\"ANCHOR\":\n",
    "        explainer_features = set(get_nonzero_features_anchor(row, clf, num_features, \n",
    "                                                  explainer, max_features, \n",
    "                                                  predicted_label_index, num_samples,\n",
    "                                                  seed))\n",
    "    elif method==\"SHAP\":\n",
    "        explainer_features = set(get_nonzero_features_shap(row, clf, num_features, \n",
    "                                                  explainer, max_features, \n",
    "                                                  predicted_label_index, num_samples,\n",
    "                                                  seed))\n",
    "    else:\n",
    "        raise Exception(\"Incorrect method selected\", method)\n",
    "#     print(idx, \n",
    "#           fidelity,\n",
    "#           list(clf_features), \n",
    "#           list(explainer_features),\n",
    "#           sum([x in clf_features for x in explainer_features])/len(explainer_features) if len(explainer_features)>0 else 0.0,\n",
    "#           sum([x in explainer_features for x in clf_features])/len(clf_features) if len(clf_features)>0 else 0.0)\n",
    "    fout.write(\"{}, {}, {}, {}\\n\".format(idx, \n",
    "                                         list(clf_features), \n",
    "                                         list(explainer_features), \n",
    "                                         sum([x in clf_features for x in explainer_features])/len(explainer_features) if len(explainer_features)>0 else 0.0,\n",
    "                                         sum([x in explainer_features for x in clf_features])/len(clf_features) if len(clf_features)>0 else 0.0))\n",
    "    return list(clf_features), list(explainer_features), fidelity\n",
    "    \n",
    "\n",
    "def evaluate_explanations_parallel(dataset_name, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, max_features, method='LIME', xlime_mode='ONE'):\n",
    "    print('Function evaluate_explanations_parallel with params = ', dataset_name, clf, classifier_type, num_samples, around_instance, seed, max_features, method, xlime_mode)\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    train_df2 = train_df.drop('class', axis=1)   #train_df2 and test_df2 is without class labels\n",
    "    test_df2 = test_df.drop('class', axis=1)\n",
    "    ordered_class_labels = sorted(list(set(train_df['class'].values)))\n",
    "    print('train_df ordered_class_labels: ', ordered_class_labels)\n",
    "    columns = list(train_df2.columns)\n",
    "    categorical_features        = [x for x in columns if '_' in str(x)]\n",
    "    categorical_feature_indices = [columns.index(x) for x in columns if '_' in str(x)]\n",
    "    categorical_features_map    = {columns.index(x):x for x in columns if '_' in str(x)}\n",
    "    \n",
    "    all_features = train_df2.columns.values\n",
    "    print('dataset:', dataset_name, 'method:', method, 'seed:', seed, 'num_samples:', num_samples, 'test size:', test_df2.shape)\n",
    "    print('all_features')\n",
    "    print(all_features)\n",
    "    fout.write('dataset: {} method: {} seed: {} num_sampes: {} test size: {}\\n'.format(dataset_name, method, seed, num_samples, test_df2.shape))\n",
    "    if method=='XLIME':\n",
    "#         discretizers = ['decile', 'eightile', 'sixile', 'quartile'] \n",
    "#         for i in range(4):\n",
    "#             try:\n",
    "        explainer = xlime.lime_tabular.LimeTabularExplainer(train_df2.values, \n",
    "                                                           categorical_features=categorical_feature_indices, \n",
    "                                                           feature_names=all_features,\n",
    "                                                           verbose=False, \n",
    "                                                           class_names=ordered_class_labels,\n",
    "                                                           mode='classification',\n",
    "                                                           random_state=RandomState(seed), \n",
    "                                                           discretizer='decile',\n",
    "#                                                            discretizer=discretizers[i],\n",
    "    #                                                        training_labels=train_df['class'].values,\n",
    "                                                           feature_selection='none'\n",
    "                                                          )\n",
    "#                 print(discretizers[i])\n",
    "#                 break\n",
    "#             except Exception as e:\n",
    "#                 print(str(e))\n",
    "#                 pass\n",
    "    elif method=='LIME':\n",
    "        print('My method is LIME')\n",
    "        discretizers = ['decile', 'eightile', 'sixile', 'quartile'] \n",
    "        for i in range(1):\n",
    "            try:\n",
    "                explainer = lime.lime_tabular.LimeTabularExplainer(train_df2.values,  \n",
    "                                                       categorical_features=categorical_feature_indices, \n",
    "                                                       feature_names=all_features,\n",
    "                                                       verbose=False, \n",
    "                                                       class_names=ordered_class_labels,\n",
    "                                                       mode='classification', \n",
    "                                                       sample_around_instance=around_instance, \n",
    "                                                       random_state=RandomState(seed),\n",
    "                                                       discretizer=discretizers[i],\n",
    "#                                                        training_labels=train_df['class'].values,\n",
    "                                                      )\n",
    "                print(explainer)\n",
    "#                 print(discretizers[i])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                pass\n",
    "    elif method=='ANCHOR':\n",
    "#          def __init__(self, class_names, feature_names, data=None,\n",
    "#                  categorical_names=None, ordinal_features=[]):\n",
    "#         explainer = anchor_tabular.AnchorTabularExplainer(dataset.class_names, dataset.feature_names, \n",
    "#                                                           dataset.data, dataset.categorical_names)\n",
    "        explainer = anchor.anchor_tabular.AnchorTabularExplainer(ordered_class_labels,\n",
    "                                                       train_data=train_df2.values,  \n",
    "                                                       categorical_names=categorical_features_map, \n",
    "                                                       feature_names=all_features,\n",
    "#                                                        verbose=False, \n",
    "#                                                        class_names=ordered_class_labels, # error\n",
    "#                                                        mode='classification',  # error\n",
    "#                                                        sample_around_instance=around_instance,  # error\n",
    "#                                                        random_state=RandomState(seed), # error\n",
    "                                                       discretizer='decile'\n",
    "                                                      )\n",
    "#         explainer.fit(dataset.train, dataset.labels_train, dataset.validation, dataset.labels_validation)\n",
    "#         split_idx = int(train_df.shape[0] * 0.75)\n",
    "#         temp_train_df = train_df[:split_idx]\n",
    "#         temp_valid_df = train_df[split_idx:]\n",
    "#         print(temp_valid_df['class'].values)\n",
    "#         explainer.fit(train_df.drop('class', axis=1).values, train_df['class'].values,\n",
    "#                       train_df.drop('class', axis=1).values, train_df['class'].values,\n",
    "#                       discretizer='decile')\n",
    "\n",
    "    elif method=='SHAP':\n",
    "        explainer = shap.KernelExplainer(clf.predict_proba, train_df2.values) #, link=<shap.common.IdentityLink object>, **kwargs)\n",
    "    else:\n",
    "        raise Exception(\"incorrect explanation method provided to evaluate_explanations:\", method)\n",
    "        \n",
    "    num_labels = len(ordered_class_labels) # 0#max(train_df['class']) - train_df.shape[1]\n",
    "    \n",
    "    # modify this based on the type of the experiments (tuning --> train_df2)/(evaluating --> test_df2)\n",
    "#     chosen_dataset = train_df2\n",
    "    chosen_dataset = test_df2\n",
    "    \n",
    "    if method in ('XLIME', 'ANCHOR'):\n",
    "        print('1', method)\n",
    "        with pathos.multiprocessing.ProcessPool(ncpus=PROCESS_COUNT) as pool:\n",
    "            ret = pool.map(evaluate_instance, zip(\n",
    "                                                  chosen_dataset.iterrows(),\n",
    "                                                  itertools.cycle([clf]),\n",
    "                                                  itertools.cycle([classifier_type]),\n",
    "                                                  itertools.cycle([chosen_dataset.shape[1]]),\n",
    "                                                  itertools.cycle([num_labels]),\n",
    "                                                  itertools.cycle([explainer]),\n",
    "                                                  itertools.cycle([num_samples]),\n",
    "                                                  itertools.cycle([xlime_mode]),\n",
    "                                                  itertools.cycle([seed]),\n",
    "                                                  itertools.cycle([ordered_class_labels]),\n",
    "                                                  itertools.cycle([method]),\n",
    "                                                  itertools.cycle([max_features])\n",
    "                                                 ),                                                                          \n",
    "                                             )\n",
    "    else:\n",
    "        print('2', method, ordered_class_labels, list(zip(\n",
    "                                                  chosen_dataset.iterrows(),\n",
    "                                                  itertools.cycle([clf]),\n",
    "                                                  itertools.cycle([classifier_type]),\n",
    "                                                  itertools.cycle([chosen_dataset.shape[1]]),\n",
    "                                                  itertools.cycle([num_labels]),\n",
    "                                                  itertools.cycle([explainer]),\n",
    "                                                  itertools.cycle([num_samples]),\n",
    "                                                  itertools.cycle([xlime_mode]),\n",
    "                                                  itertools.cycle([seed]),\n",
    "                                                  itertools.cycle([ordered_class_labels]),\n",
    "                                                  itertools.cycle([method]),\n",
    "                                                  itertools.cycle([max_features])\n",
    "                                                 )))\n",
    "        \n",
    "        ret = map(evaluate_instance, zip(\n",
    "                                                  chosen_dataset.iterrows(),\n",
    "                                                  itertools.cycle([clf]),\n",
    "                                                  itertools.cycle([classifier_type]),\n",
    "                                                  itertools.cycle([chosen_dataset.shape[1]]),\n",
    "                                                  itertools.cycle([num_labels]),\n",
    "                                                  itertools.cycle([explainer]),\n",
    "                                                  itertools.cycle([num_samples]),\n",
    "                                                  itertools.cycle([xlime_mode]),\n",
    "                                                  itertools.cycle([seed]),\n",
    "                                                  itertools.cycle([ordered_class_labels]),\n",
    "                                                  itertools.cycle([method]),\n",
    "                                                  itertools.cycle([max_features])\n",
    "                                                 ),                                                                          \n",
    "                                             )\n",
    "    all_clf_features, all_exp_features, fidelities = zip(*ret)\n",
    "    print('--- Returned ', all_clf_features, all_exp_features, fidelities)\n",
    "    return all_clf_features, all_exp_features, fidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:15:40.102939Z",
     "start_time": "2020-10-28T08:15:39.867062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "import rbo\n",
    "def analyze_outputs(all_features, initial_all_clf_features, initial_all_exp_features, fidelities):\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    if fidelity_division:\n",
    "        all_clf_features = [x for x,fidel in zip(initial_all_clf_features, fidelities) if fidel]\n",
    "        all_exp_features = [x for x,fidel in zip(initial_all_exp_features, fidelities) if fidel]\n",
    "    else:\n",
    "        all_clf_features = initial_all_clf_features\n",
    "        all_exp_features = initial_all_exp_features\n",
    "        \n",
    "    print(len(all_clf_features), sum(fidelities))\n",
    "#     print('shapes:', len(all_clf_features), len(all_exp_features))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([[i+1] for i,x in enumerate(all_features)])\n",
    "#     print('unique 1:', set(itertools.chain(*all_clf_features)))\n",
    "#     print('unique 2:', set(itertools.chain(*all_exp_features)))\n",
    "    print(classification_report(mlb.transform(all_clf_features), mlb.transform(all_exp_features), output_dict=True)['samples avg'])\n",
    "    fout.write(str(classification_report(mlb.transform(all_clf_features), mlb.transform(all_exp_features), output_dict=True)['samples avg'])+'\\n')\n",
    "\n",
    "    print('f0.5-score:', precision_recall_fscore_support(mlb.transform(all_clf_features), mlb.transform(all_exp_features), beta=0.5, average='samples')[2])\n",
    "    fout.write('f0.5-score: ' + str(precision_recall_fscore_support(mlb.transform(all_clf_features), mlb.transform(all_exp_features), beta=0.5, average='samples')[2])+'\\n')\n",
    "\n",
    "    print('Jaccard similarity:', jaccard_score(mlb.transform(all_clf_features), mlb.transform(all_exp_features), \n",
    "                                               average='samples'))\n",
    "    fout.write('Jaccard similarity: ' + str(jaccard_score(mlb.transform(all_clf_features), mlb.transform(all_exp_features), \n",
    "                                               average='samples'))+'\\n')\n",
    "    # TODO 1: add RBO to this method\n",
    "    # TODO 2: Are SHAP, Anchor outputs ordered?\n",
    "    rbo_scores = []\n",
    "    rbo_scores2 = []\n",
    "    for clf_features, exp_features in zip(all_clf_features, all_exp_features):\n",
    "#         rbo_score = rbo.RankingSimilarity(clf_features, exp_features).rbo()\n",
    "#         rbo_scores.append(rbo_score)\n",
    "#         print('RBO: {} {} {}'.format(rbo_score, clf_features, exp_features))\n",
    "        if len(clf_features)>0 and len(exp_features)>0:\n",
    "            rbo_score2 = rbo(clf_features, exp_features, p=0.5)\n",
    "#             rbo_scores2.append(rbo_score2.min + rbo_score2.res/2)\n",
    "            rbo_scores2.append(rbo_score2.ext)\n",
    "        else:\n",
    "            rbo_score2 = 0.\n",
    "            rbo_scores2.append(0.)\n",
    "#         print('RBO: {}'.format(rbo_score2))\n",
    "#     print('RBO AVG: {}'.format(sum(rbo_scores)/len(rbo_scores)))\n",
    "    print('RBO2 AVG: {}'.format(sum(rbo_scores2)/len(rbo_scores2)))\n",
    "    fout.write('RBO2 AVG: {}\\n'.format(sum(rbo_scores2)/len(rbo_scores2)))\n",
    "    print('FIDELITY AVG: {}'.format(sum(fidelities)/len(fidelities)))\n",
    "    fout.write('FIDELITY AVG: {}\\n'.format(sum(fidelities)/len(fidelities)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:18:02.252632Z",
     "start_time": "2020-10-28T08:15:40.104356Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1000, 6000, 1000)\n",
      "Barbe experiment started with iterations =  0  Dataset:  glass\n",
      "*** Classifier =  DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160)\n",
      "Function evaluate_explanations_parallel with params =  glass DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160) dt 1000 True 1 5 LIME ONE\n",
      "train_df ordered_class_labels:  [1, 2, 3, 5, 6, 7]\n",
      "dataset: glass method: LIME seed: 1 num_samples: 1000 test size: (54, 9)\n",
      "all_features\n",
      "[1 2 3 4 5 6 7 8 9]\n",
      "My method is LIME\n",
      "************** REGRET *********************\n",
      "************** REGRET ********************* feature_names [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "In BaseDiscretizer: self.to_discretize =  [0, 1, 2, 3, 4, 5, 6, 7, 8] bins =  [array([-0.74802348, -0.64307258, -0.53547307, -0.38549592, -0.23287016,\n",
      "       -0.11732484,  0.07834072,  0.38326116,  1.19207836]), array([-0.92905471, -0.70879808, -0.53871101, -0.38086043, -0.14959096,\n",
      "        0.09880957,  0.34598646,  0.74979029,  1.39710006]), array([-1.88549095, -0.36711628,  0.46658388,  0.5642941 ,  0.60295642,\n",
      "        0.62615381,  0.65356891,  0.73019059]), array([-0.93171963, -0.63212302, -0.49162254, -0.35112206, -0.13830516,\n",
      "        0.09930595,  0.25013734,  0.50014555,  1.32661896]), array([-0.98200763, -0.54761902, -0.19776712,  0.055843  ,  0.2288181 ,\n",
      "        0.3835853 ,  0.51234121,  0.63329372,  0.92071852]), array([-0.70492202, -0.68300024, -0.50351573, -0.12125481,  0.05548949,\n",
      "        0.07604115,  0.11714448,  0.17194891,  0.21442234]), array([-0.68423214, -0.57831623, -0.43863003, -0.28052367, -0.20070298,\n",
      "       -0.05641174,  0.04950418,  0.37185695,  0.96590707]), array([-0.37468472, -0.35186128,  0.82544755]), array([-0.57482738,  0.10952179,  1.10493877,  1.91371506])]\n",
      "Hi There:  <lime.discretize.DecileDiscretizer object at 0x7f2c0e8b3790>\n",
      "self.categorical_features =  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "self.feature_selection =  auto\n",
      "self.class_names =  [1, 2, 3, 5, 6, 7]\n",
      "self.feature_values =  {0: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 1: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 2: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], 3: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 4: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 5: [0.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 6: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0], 7: [0.0, 2.0, 3.0], 8: [0.0, 1.0, 2.0, 3.0, 4.0]}\n",
      "<lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>\n",
      "2 LIME [1, 2, 3, 5, 6, 7] [((143, 1   -0.363645\n",
      "2   -0.535040\n",
      "3    0.553750\n",
      "4    0.719161\n",
      "5    0.040236\n",
      "6    0.199351\n",
      "7   -0.515381\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 143, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((168, 1   -0.506007\n",
      "2   -0.706351\n",
      "3   -1.885491\n",
      "4    0.801808\n",
      "5    1.548891\n",
      "6    0.624085\n",
      "7    1.011957\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 168, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((71, 1    0.096550\n",
      "2    0.248095\n",
      "3    0.834930\n",
      "4   -0.355254\n",
      "5   -0.948193\n",
      "6    0.034938\n",
      "7   -0.407930\n",
      "8   -0.374685\n",
      "9    2.743229\n",
      "Name: 71, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((54, 1   -0.135203\n",
      "2   -0.278074\n",
      "3    0.089802\n",
      "4   -0.313931\n",
      "5    0.378383\n",
      "6   -0.006165\n",
      "7    0.129325\n",
      "8   -0.374685\n",
      "9    0.358376\n",
      "Name: 54, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((88, 1   -0.664924\n",
      "2   -0.522804\n",
      "3    0.574838\n",
      "4    0.078644\n",
      "5    0.261332\n",
      "6    0.117144\n",
      "7   -0.561431\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 88, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((131, 1    2.632587\n",
      "2    0.321514\n",
      "3   -1.885491\n",
      "4   -0.169298\n",
      "5   -1.884600\n",
      "6   -0.444601\n",
      "7    3.521704\n",
      "8   -0.374685\n",
      "9    0.462065\n",
      "Name: 131, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((124, 1    1.185788\n",
      "2   -0.290310\n",
      "3    0.701370\n",
      "4   -0.603196\n",
      "5    0.079253\n",
      "6    0.034938\n",
      "7   -0.254428\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 124, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((21, 1    0.487219\n",
      "2    1.630817\n",
      "3    0.750576\n",
      "4   -2.380114\n",
      "5   -0.870159\n",
      "6   -0.663819\n",
      "7    0.113975\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 21, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((14, 1   -0.184864\n",
      "2   -1.012263\n",
      "3    0.638104\n",
      "4   -0.272607\n",
      "5    0.781558\n",
      "6    0.089742\n",
      "7   -0.269779\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 14, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((151, 1    1.020250\n",
      "2    1.080175\n",
      "3    0.856019\n",
      "4   -1.264375\n",
      "5   -1.546453\n",
      "6   -0.704922\n",
      "7    0.490053\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 151, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((196, 1   -0.870190\n",
      "2    0.529534\n",
      "3   -1.885491\n",
      "4    2.268799\n",
      "5    0.703524\n",
      "6   -0.513107\n",
      "7    0.428652\n",
      "8    1.165897\n",
      "9   -0.471138\n",
      "Name: 196, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((123, 1   -0.370266\n",
      "2    0.052311\n",
      "3    0.560779\n",
      "4    0.553866\n",
      "5   -0.219877\n",
      "6    0.144547\n",
      "7   -0.661207\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 123, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((138, 1   -0.479521\n",
      "2   -0.792006\n",
      "3    0.588897\n",
      "4    0.202615\n",
      "5    0.872598\n",
      "6    0.199351\n",
      "7   -0.730283\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 138, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((111, 1    3.046432\n",
      "2   -2.957863\n",
      "3   -1.885491\n",
      "4   -1.429670\n",
      "5    0.508440\n",
      "6   -0.704922\n",
      "7    4.688314\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 111, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((51, 1    0.354789\n",
      "2   -0.290310\n",
      "3    0.455337\n",
      "4   -0.334593\n",
      "5   -0.427967\n",
      "6    0.117144\n",
      "7    0.221426\n",
      "8   -0.374685\n",
      "9    0.565755\n",
      "Name: 51, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((112, 1    3.172240\n",
      "2   -0.975553\n",
      "3   -1.885491\n",
      "4   -1.594965\n",
      "5   -0.870159\n",
      "6   -0.622715\n",
      "7    4.258510\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 112, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((9, 1   -0.211350\n",
      "2   -0.535040\n",
      "3    0.645133\n",
      "4   -0.169298\n",
      "5    0.391389\n",
      "6    0.076041\n",
      "7   -0.346529\n",
      "8   -0.374685\n",
      "9    0.565755\n",
      "Name: 9, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((19, 1   -0.277565\n",
      "2   -0.510567\n",
      "3    0.602956\n",
      "4    0.512543\n",
      "5    0.053242\n",
      "6    0.034938\n",
      "7   -0.315829\n",
      "8   -0.374685\n",
      "9    0.150997\n",
      "Name: 19, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((16, 1   -0.115338\n",
      "2   -0.926607\n",
      "3    0.694340\n",
      "4   -0.582535\n",
      "5    0.547456\n",
      "6    0.130846\n",
      "7   -0.116277\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 16, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((86, 1   -0.827151\n",
      "2   -0.241365\n",
      "3    0.567809\n",
      "4    0.057982\n",
      "5    0.729536\n",
      "6   -0.184280\n",
      "7   -0.630507\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 86, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((0, 1    0.934171\n",
      "2    0.248095\n",
      "3    1.270760\n",
      "4   -0.706506\n",
      "5   -1.182295\n",
      "6   -0.622715\n",
      "7   -0.077902\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 0, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((105, 1    2.172393\n",
      "2   -2.431694\n",
      "3   -1.885491\n",
      "4    0.905118\n",
      "5   -0.649063\n",
      "6    0.404868\n",
      "7    3.368203\n",
      "8   -0.374685\n",
      "9    2.950608\n",
      "Name: 105, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((44, 1   -0.108717\n",
      "2   -0.865425\n",
      "3    0.525632\n",
      "4   -0.520549\n",
      "5    0.339366\n",
      "6    0.144547\n",
      "7   -0.070227\n",
      "8   -0.374685\n",
      "9    2.535851\n",
      "Name: 44, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((178, 1    0.033646\n",
      "2    1.251486\n",
      "3   -0.310880\n",
      "4    0.367910\n",
      "5   -0.401956\n",
      "6   -0.704922\n",
      "7    0.313526\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 178, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((48, 1    1.338083\n",
      "2   -0.278074\n",
      "3    0.764635\n",
      "4   -1.347023\n",
      "5   -0.909176\n",
      "6   -0.526808\n",
      "7    0.896831\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 48, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((153, 1   -0.691410\n",
      "2   -0.021108\n",
      "3    0.504543\n",
      "4   -0.458564\n",
      "5    0.001219\n",
      "6    0.103443\n",
      "7   -0.407930\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 153, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((70, 1   -0.810597\n",
      "2    1.740945\n",
      "3    0.694340\n",
      "4    0.615852\n",
      "5   -1.065244\n",
      "6   -0.485704\n",
      "7   -1.144736\n",
      "8   -0.374685\n",
      "9    0.669444\n",
      "Name: 70, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((169, 1    0.579920\n",
      "2   -0.204655\n",
      "3   -1.885491\n",
      "4    0.657175\n",
      "5    0.443411\n",
      "6   -0.060970\n",
      "7    1.894590\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 169, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((38, 1    1.304975\n",
      "2    0.945574\n",
      "3    0.799783\n",
      "4   -2.008201\n",
      "5   -1.195300\n",
      "6   -0.554210\n",
      "7    0.551453\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 38, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((150, 1   -0.509318\n",
      "2   -0.363729\n",
      "3    0.539691\n",
      "4    0.657175\n",
      "5   -0.271899\n",
      "6    0.117144\n",
      "7   -0.361879\n",
      "8   -0.374685\n",
      "9    1.187890\n",
      "Name: 150, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((156, 1   -0.542426\n",
      "2   -0.033344\n",
      "3    0.497514\n",
      "4   -0.334593\n",
      "5   -0.063809\n",
      "6    0.007536\n",
      "7   -0.154653\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 156, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((163, 1   -1.009242\n",
      "2    0.700844\n",
      "3   -0.001582\n",
      "4    4.252335\n",
      "5   -3.640362\n",
      "6    1.596864\n",
      "7   -2.288321\n",
      "8    3.809612\n",
      "9   -0.574827\n",
      "Name: 163, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((167, 1    0.497151\n",
      "2   -0.975553\n",
      "3   -1.885491\n",
      "4    0.429895\n",
      "5    1.379818\n",
      "6   -0.184280\n",
      "7    2.055766\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 167, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((197, 1   -0.304051\n",
      "2    1.545161\n",
      "3   -1.885491\n",
      "4    1.855562\n",
      "5    0.768552\n",
      "6   -0.704922\n",
      "7    0.075599\n",
      "8    0.880604\n",
      "9   -0.574827\n",
      "Name: 197, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((188, 1    1.417541\n",
      "2    1.740945\n",
      "3   -0.338998\n",
      "4    1.277031\n",
      "5   -3.159153\n",
      "6    0.336362\n",
      "7    0.697280\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 188, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((64, 1    1.169234\n",
      "2    0.052311\n",
      "3    0.743547\n",
      "4   -1.119742\n",
      "5   -0.883165\n",
      "6   -0.458302\n",
      "7    0.582154\n",
      "8   -0.374685\n",
      "9    0.150997\n",
      "Name: 64, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((68, 1    1.103019\n",
      "2   -0.388202\n",
      "3    0.631074\n",
      "4   -1.119742\n",
      "5   -0.636057\n",
      "6   -0.389797\n",
      "7    0.743330\n",
      "8   -0.374685\n",
      "9    1.084201\n",
      "Name: 68, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((145, 1    0.066753\n",
      "2   -0.718587\n",
      "3    0.694340\n",
      "4   -0.417240\n",
      "5   -0.154848\n",
      "6    0.144547\n",
      "7   -0.131627\n",
      "8   -0.374685\n",
      "9    3.054297\n",
      "Name: 145, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((158, 1   -0.141824\n",
      "2    0.113493\n",
      "3    0.511573\n",
      "4    0.161291\n",
      "5   -0.844148\n",
      "6    0.089742\n",
      "7   -0.047202\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 158, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((194, 1   -0.449725\n",
      "2    1.373851\n",
      "3   -1.885491\n",
      "4    1.111736\n",
      "5    0.781558\n",
      "6   -0.704922\n",
      "7   -0.254428\n",
      "8    2.611381\n",
      "9    0.150997\n",
      "Name: 194, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((42, 1   -0.131892\n",
      "2   -0.278074\n",
      "3    0.497514\n",
      "4   -0.231283\n",
      "5    0.092259\n",
      "6    0.103443\n",
      "7   -0.200703\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 42, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((160, 1    0.043578\n",
      "2   -0.131236\n",
      "3    0.462366\n",
      "4    0.202615\n",
      "5   -0.714091\n",
      "6    0.062340\n",
      "7    0.106300\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 160, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((87, 1   -0.575533\n",
      "2   -0.045581\n",
      "3    0.567809\n",
      "4    0.161291\n",
      "5   -0.050803\n",
      "6    0.213052\n",
      "7   -0.592131\n",
      "8   -0.374685\n",
      "9    0.462065\n",
      "Name: 87, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((113, 1    0.242223\n",
      "2    0.027838\n",
      "3    0.806812\n",
      "4   -0.375916\n",
      "5   -0.180860\n",
      "6    0.076041\n",
      "7   -0.492355\n",
      "8   -0.374685\n",
      "9    0.876822\n",
      "Name: 113, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((63, 1    1.351326\n",
      "2    0.896628\n",
      "3    0.792753\n",
      "4   -1.367684\n",
      "5   -1.741537\n",
      "6   -0.704922\n",
      "7    0.643554\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 63, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((79, 1   -0.757625\n",
      "2   -0.755297\n",
      "3    0.588897\n",
      "4    0.946441\n",
      "5    0.222315\n",
      "6    0.240454\n",
      "7   -0.676557\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 79, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((133, 1   -0.062366\n",
      "2    0.333750\n",
      "3    0.877107\n",
      "4    0.202615\n",
      "5   -1.143278\n",
      "6    0.034938\n",
      "7   -0.492355\n",
      "8   -0.374685\n",
      "9    0.980512\n",
      "Name: 133, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((175, 1    0.993764\n",
      "2   -0.571750\n",
      "3   -1.653517\n",
      "4    0.140630\n",
      "5    0.911615\n",
      "6   -0.526808\n",
      "7    1.856215\n",
      "8   -0.374685\n",
      "9    2.328472\n",
      "Name: 175, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((166, 1    1.099708\n",
      "2   -2.945627\n",
      "3   -0.683444\n",
      "4    0.243939\n",
      "5    0.976643\n",
      "6    0.089742\n",
      "7    2.124842\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 166, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((94, 1   -0.628505\n",
      "2   -0.889898\n",
      "3    0.455337\n",
      "4    0.099306\n",
      "5    0.768552\n",
      "6    0.213052\n",
      "7   -0.469330\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 94, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((24, 1   -0.327227\n",
      "2   -0.070054\n",
      "3    0.574838\n",
      "4   -0.603196\n",
      "5    0.209310\n",
      "6   -0.019867\n",
      "7   -0.323504\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 24, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((195, 1   -0.906609\n",
      "2    0.859919\n",
      "3   -1.885491\n",
      "4    2.558064\n",
      "5    0.911615\n",
      "6   -0.595313\n",
      "7    0.167700\n",
      "8    0.785507\n",
      "9   -0.056381\n",
      "Name: 195, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((147, 1   -0.691410\n",
      "2   -0.131236\n",
      "3    0.595927\n",
      "4   -0.210622\n",
      "5   -0.024792\n",
      "6    0.062340\n",
      "7   -0.400255\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 147, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5), ((32, 1   -0.145135\n",
      "2   -0.718587\n",
      "3    0.560779\n",
      "4   -0.437902\n",
      "5    0.365377\n",
      "6    0.130846\n",
      "7   -0.223728\n",
      "8   -0.203509\n",
      "9    1.706337\n",
      "Name: 32, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)]\n",
      "Evaluating:  ((143, 1   -0.363645\n",
      "2   -0.535040\n",
      "3    0.553750\n",
      "4    0.719161\n",
      "5    0.040236\n",
      "6    0.199351\n",
      "7   -0.515381\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 143, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 3, 7, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((168, 1   -0.506007\n",
      "2   -0.706351\n",
      "3   -1.885491\n",
      "4    0.801808\n",
      "5    1.548891\n",
      "6    0.624085\n",
      "7    1.011957\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 168, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "3 5\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 3, 2, 7]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating:  ((71, 1    0.096550\n",
      "2    0.248095\n",
      "3    0.834930\n",
      "4   -0.355254\n",
      "5   -0.948193\n",
      "6    0.034938\n",
      "7   -0.407930\n",
      "8   -0.374685\n",
      "9    2.743229\n",
      "Name: 71, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((54, 1   -0.135203\n",
      "2   -0.278074\n",
      "3    0.089802\n",
      "4   -0.313931\n",
      "5    0.378383\n",
      "6   -0.006165\n",
      "7    0.129325\n",
      "8   -0.374685\n",
      "9    0.358376\n",
      "Name: 54, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "0 1\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((88, 1   -0.664924\n",
      "2   -0.522804\n",
      "3    0.574838\n",
      "4    0.078644\n",
      "5    0.261332\n",
      "6    0.117144\n",
      "7   -0.561431\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 88, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 3, 7, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((131, 1    2.632587\n",
      "2    0.321514\n",
      "3   -1.885491\n",
      "4   -0.169298\n",
      "5   -1.884600\n",
      "6   -0.444601\n",
      "7    3.521704\n",
      "8   -0.374685\n",
      "9    0.462065\n",
      "Name: 131, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((124, 1    1.185788\n",
      "2   -0.290310\n",
      "3    0.701370\n",
      "4   -0.603196\n",
      "5    0.079253\n",
      "6    0.034938\n",
      "7   -0.254428\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 124, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "0 1\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((21, 1    0.487219\n",
      "2    1.630817\n",
      "3    0.750576\n",
      "4   -2.380114\n",
      "5   -0.870159\n",
      "6   -0.663819\n",
      "7    0.113975\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 21, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "0 1\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((14, 1   -0.184864\n",
      "2   -1.012263\n",
      "3    0.638104\n",
      "4   -0.272607\n",
      "5    0.781558\n",
      "6    0.089742\n",
      "7   -0.269779\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 14, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "0 1\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((151, 1    1.020250\n",
      "2    1.080175\n",
      "3    0.856019\n",
      "4   -1.264375\n",
      "5   -1.546453\n",
      "6   -0.704922\n",
      "7    0.490053\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 151, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "0 1\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((196, 1   -0.870190\n",
      "2    0.529534\n",
      "3   -1.885491\n",
      "4    2.268799\n",
      "5    0.703524\n",
      "6   -0.513107\n",
      "7    0.428652\n",
      "8    1.165897\n",
      "9   -0.471138\n",
      "Name: 196, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "5 7\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 5]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((123, 1   -0.370266\n",
      "2    0.052311\n",
      "3    0.560779\n",
      "4    0.553866\n",
      "5   -0.219877\n",
      "6    0.144547\n",
      "7   -0.661207\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 123, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 3, 7, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((138, 1   -0.479521\n",
      "2   -0.792006\n",
      "3    0.588897\n",
      "4    0.202615\n",
      "5    0.872598\n",
      "6    0.199351\n",
      "7   -0.730283\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 138, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 3, 7, 9]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n",
      "Evaluating:  ((111, 1    3.046432\n",
      "2   -2.957863\n",
      "3   -1.885491\n",
      "4   -1.429670\n",
      "5    0.508440\n",
      "6   -0.704922\n",
      "7    4.688314\n",
      "8   -0.374685\n",
      "9   -0.574827\n",
      "Name: 111, dtype: float64), DecisionTreeClassifier(max_depth=5,\n",
      "                       random_state=RandomState(MT19937) at 0x7F2CC67B7160), 'dt', 9, 6, <lime.lime_tabular.LimeTabularExplainer object at 0x7f2c0e9211d0>, 1000, 'ONE', 1, [1, 2, 3, 5, 6, 7], 'LIME', 5)\n",
      "1 2\n",
      "Inside: get_features_dt, feature =  [ 7  3  6  0  0 -2 -2  8 -2 -2  0  1 -2 -2 -2  2  1  6 -2 -2 -2  6  8 -2\n",
      " -2  1 -2 -2  4  7 -2 -2 -2]\n",
      "final_features =  [8, 4, 7, 1]\n",
      "----\n",
      "** classes  [0, 1, 2, 3, 4, 5]\n",
      "Caller 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-bf12db2047d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#             # evaluate LIME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mouputs_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_explanations_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maround_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_explanation_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-b2695b7ec953>\u001b[0m in \u001b[0;36mevaluate_explanations_parallel\u001b[0;34m(dataset_name, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, max_features, method, xlime_mode)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                                  ),                                                                          \n\u001b[1;32m    217\u001b[0m                                              )\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mall_clf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_exp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_clf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_exp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfidelities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-b2695b7ec953>\u001b[0m in \u001b[0;36mevaluate_instance\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                   \u001b[0mexplainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                                   \u001b[0mpredicted_label_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlime_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                                   seed)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"ANCHOR\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-6fffb3f4208a>\u001b[0m in \u001b[0;36mget_nonzero_features_lime\u001b[0;34m(row, clf, num_features, explainer, num_labels, max_valid_features, predicted_label_index, num_samples, xlime_mode, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_valid_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_label_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                num_samples=num_samples)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#         print(x.local_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         fidelity = 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/barbetest/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor, barbe_mode)\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mfeature_selection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0mneighborhood_data_sd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msd_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                     ohe=ohe)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/barbetest/lime/lime_base.py\u001b[0m in \u001b[0;36mexplain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor, neighborhood_data_sd, ohe)\u001b[0m\n\u001b[1;32m    322\u001b[0m                                                \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                                                \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                                                feature_selection)\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigDirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/barbetest/lime/lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mn_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'highest_weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             return self.feature_selection(data, labels, weights,\n\u001b[0;32m--> 270\u001b[0;31m                                           num_features, n_method)\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     def explain_instance_with_data(self,\n",
      "\u001b[0;32m~/barbetest/lime/lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'forward_selection'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'highest_weights'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             clf = Ridge(alpha=0, fit_intercept=True,\n",
      "\u001b[0;32m~/barbetest/lime/lime_base.py\u001b[0m in \u001b[0;36mforward_selection\u001b[0;34m(self, data, labels, weights, num_features)\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 clf.fit(data[:, used_features + [feature]], labels,\n\u001b[0;32m--> 195\u001b[0;31m                         sample_weight=weights)\n\u001b[0m\u001b[1;32m    196\u001b[0m                 score = clf.score(data[:, used_features + [feature]],\n\u001b[1;32m    197\u001b[0m                                   \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \"\"\"\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 return_intercept=False, check_input=False, **params)\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36m_ridge_regression\u001b[0;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solve_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;31m# use SVD solver if matrix is singular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/barbe-conda-venv/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36m_solve_cholesky\u001b[0;34m(X, y, alpha)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mone_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mone_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theNotebook = 'final_anchor' # get_notebook_name().split('.')[0]\n",
    "fout = open('{}.log'.format(theNotebook), 'a', 1)\n",
    "samples_range = range(1000, 6000, 1000)\n",
    "print(samples_range)\n",
    "# samples_range = range(100, 1100, 100)\n",
    "NUM_ITERATIONS = 5\n",
    "classifier_type = 'dt'\n",
    "around_instance = True\n",
    "_reload_libs()\n",
    "xlime_mode = [\"FOURTEEN\"]\n",
    "iteration_seed = 0\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    iteration_seed += 1\n",
    "    for dataset, dataset_info in datasets_info_dict.items():\n",
    "        print('Barbe experiment started with iterations = ', i, ' Dataset: ', dataset)\n",
    "        seed = iteration_seed\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            clf = dataset_info['{}_clf'.format(classifier_type)]\n",
    "            print('*** Classifier = ', clf)\n",
    "            train_df = dataset_info['train_df']    #Train_df contains class label\n",
    "            test_df  = dataset_info['test_df']     #Test_df contains class label\n",
    "            all_features = train_df.drop('class', axis=1).columns.values\n",
    "            \n",
    "# #             # evaluate XLIME\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'XLIME', xlime_mode)\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "#             # evaluate LIME\n",
    "            for num_samples in samples_range:\n",
    "                ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'LIME')\n",
    "                #analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "# #             # evaluate SHAP\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'SHAP')\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp)\n",
    "            \n",
    "# #             # evaluate Anchors\n",
    "#             for num_samples in samples_range:\n",
    "#                 ouputs_clf, outputs_exp, fidelities = evaluate_explanations_parallel(dataset, clf, train_df, test_df, classifier_type, num_samples, around_instance, seed, info['max_explanation_size'], 'ANCHOR')\n",
    "#                 analyze_outputs(all_features, ouputs_clf, outputs_exp, fidelities)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('EXCEPTION:', str(e))\n",
    "            fout.write('EXCEPTION' + str(e)+'\\n')\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import rbo\n",
    "rbo_dict(dict(a=1, b=2, c=1, d=3), dict(a=1, b=2, c=2, d=3), p=.9, sort_ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fourteen and then fifteen: 0.5, and then 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T08:18:02.254330Z",
     "start_time": "2020-10-28T08:15:32.565Z"
    }
   },
   "outputs": [],
   "source": [
    "### 0.1 * softmax\n",
    "np.arange(1.,0,-.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>94</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>170</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  height  weight\n",
       "0    3      94      31\n",
       "1   29     170     115"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'age':    [ 3,  29],\n",
    "                   'height': [94, 170],\n",
    "                   'weight': [31, 115]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'height', 'weight'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3,  94,  31],\n",
       "       [ 29, 170, 115]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
